"""
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# âš ï¸ WARNING: CORE VALUATION LOGIC - DO NOT MODIFY WITHOUT APPROVAL
# âš ï¸ WARNING: CORE VALUATION LOGIC - DO NOT MODIFY WITHOUT APPROVAL
# âš ï¸ WARNING: CORE VALUATION LOGIC - DO NOT MODIFY WITHOUT APPROVAL
# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

Valuation History Fetcher (å†å²ä¼°å€¼è·å–å™¨)
========================================

åŠŸèƒ½è¯´æ˜:
1. **å…¨å¸‚åœºè¦†ç›–**: ä¸º Aè‚¡ (CN), æ¸¯è‚¡ (HK), ç¾è‚¡ (US) æä¾›ç»Ÿä¸€çš„ä¼°å€¼æŒ‡æ ‡è·å–æ–¹æ¡ˆã€‚
2. **å¤šæºèåˆ**: ç»“åˆå®˜æ–¹æ•°æ®æº (AkShare, Futu) å’Œå•†ä¸šæ•°æ®æº (FMP, Yahoo) ä»¥åŠæœ¬åœ°æ¨å¯¼èƒ½åŠ›ã€‚
3. **æŒ‡æ ‡è¦†ç›–**: å¸‚ç›ˆç‡ (PE TTM), é™æ€å¸‚ç›ˆç‡ (PE Static), å¸‚å‡€ç‡ (PB), è‚¡æ¯ç‡ (Dividend Yield)ã€‚

æ¨¡å—æ¶æ„:
========================================

I. Core Fetchers (æ ¸å¿ƒè·å–å™¨)
----------------------------------------
è´Ÿè´£ä»å¤–éƒ¨ APIè·å–åŸå§‹æ•°æ®ï¼Œå¤„ç†ç½‘ç»œè¯·æ±‚ã€é‰´æƒå’ŒåŸºç¡€æ¸…æ´—ã€‚

| åŒºåŸŸ | å‡½æ•°å | æ•°æ®æº | å…³é”®ç‰¹æ€§ |
| :--- | :--- | :--- | :--- |
| **CN** | `fetch_cn_valuation_history` | AkShare (`stock_value_em`) | ä¸œæ–¹è´¢å¯Œå®˜æ–¹æ•°æ®ï¼ŒåŒ…å« PE-TTM/Static, PBã€‚ |
| **CN** | `fetch_cn_dividend_yield` | AkShare (`stock_fhps`) | å¤æ‚é€»è¾‘ï¼šåŸºäº"æŠ¥å‘ŠæœŸ"è®¡ç®— TTM åˆ†çº¢ï¼Œé™¤ä»¥æœ€æ–°æ”¶ç›˜ä»·ã€‚ |
| **HK** | `fetch_hk_valuation_futu` | **Futu OpenD** | **æ ¸å¿ƒé€»è¾‘**ã€‚é€šè¿‡ Socket è¿æ¥æœ¬åœ° OpenDï¼Œè·å–ç²¾å‡†çš„å†å² PE/PBã€‚åŒ…å«è‡ªåŠ¨é‡è¯•å’Œè¿æ¥ä¿æ´»ã€‚ |
| **HK** | `fetch_hk_dividend_yield` | Yahoo Finance | è¡¥å…… Futu ç¼ºå¤±çš„å®æ—¶è‚¡æ¯ç‡å­—æ®µã€‚ |
| **HK** | `fetch_hk_valuation_baidu` | Baidu Stock | (Legacy) ä»…ä½œä¸º Futu ä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆã€‚ |
| **US** | `fetch_us_valuation_fmp` | **FMP Cloud** | å•†ä¸šçº§ APIï¼Œæä¾›é•¿è¾¾ 30 å¹´çš„æ—¥çº¿çº§ PE/PB å†å²ã€‚ |
| **US** | `fetch_us_valuation_yf` | Yahoo Finance | (Realtime) ä»…ç”¨äºè·å–ç¾è‚¡ç›˜ä¸­/æ”¶ç›˜åçš„ç¬æ—¶å¿«ç…§ã€‚ |

II. Data Persistence & Alignment (æŒä¹…åŒ–ä¸å¯¹é½)
----------------------------------------
è´Ÿè´£å°†å¤šæºå¼‚æ„æ•°æ®å†™å…¥ `MarketDataDaily` è¡¨ï¼Œæ ¸å¿ƒéš¾é¢˜æ˜¯è§£å†³**æ—¶é—´æˆ³å¯¹é½**ã€‚

- **Futu å¯¹é½é€»è¾‘** (`save_hk_valuation_futu`):
  Futu Kçº¿æ—¶é—´æˆ³é€šå¸¸ä¸º `00:00:00`ï¼Œè€Œæœ¬åœ°æ•°æ®åº“å¯èƒ½å­˜å‚¨ä¸º `16:00:00` (æ”¶ç›˜æ—¶é—´)ã€‚
  ç®—æ³•æ”¯æŒ `+/- 5 Days` çš„æ¨¡ç³ŠåŒ¹é…ï¼Œä¼˜å…ˆåŒ¹é… exact matchï¼Œå…¶æ¬¡å¯»æ‰¾æœ€è¿‘çš„äº¤æ˜“æ—¥ï¼Œç¡®ä¿ä¼°å€¼æ•°æ®èƒ½æŒ‚è½½åˆ°æ­£ç¡®çš„ä»·æ ¼è®°å½•ä¸Šã€‚

- **å¢é‡æ›´æ–°**:
  æ‰€æœ‰ä¿å­˜å‡½æ•°å‡è®¾è®¡ä¸º Idempotent (å¹‚ç­‰)ï¼Œæ”¯æŒåå¤è¿è¡Œã€‚ä»…æ›´æ–°ç¼ºå¤±æˆ–å˜åŠ¨çš„å­—æ®µ (PE/PB)ï¼Œä¸ç ´åç°æœ‰çš„ OHLCV æ•°æ®ã€‚

III. US Market Logic & ADR Handling (ç¾è‚¡æ ¸å¿ƒé€»è¾‘)
----------------------------------------
ç¾è‚¡ä¼°å€¼è·å–åŒ…å«å¤æ‚çš„**æ··åˆç­–ç•¥ (Hybrid Strategy)** å’Œ **ADR è´§å¸å¯¹é½**ã€‚

1. **æ··åˆè·å–ç­–ç•¥**:
   - ä¼˜å…ˆä½¿ç”¨ `FMP Cloud` API è·å–é•¿å‘¨æœŸçš„å†å²æ¯æ—¥ PEã€‚
   - å®æ—¶æ•°æ®ä½¿ç”¨ `Yahoo Finance` è¡¥å……ã€‚
   - å½“ API ç¼ºå¤±æ—¶ï¼Œå›é€€åˆ°**æœ¬åœ°æ¨å¯¼å¼•æ“**ã€‚

2. **ADR è´§å¸è‡ªåŠ¨è½¬æ¢ (Currency Alignment)**
   - **åœºæ™¯**: éç¾ä¼ä¸šåœ¨ç¾å›½ä¸Šå¸‚ (ADR)ï¼Œè´¢æŠ¥è´§å¸é€šå¸¸ä¸ºæœ¬å¸ (CNY, TWD, JPY)ï¼Œè€Œè‚¡ä»·ä¸ºç¾å…ƒ (USD)ã€‚
   - **é€»è¾‘**: è‹¥æ£€æµ‹åˆ° `EPS Currency != Price Currency`ï¼Œè‡ªåŠ¨åº”ç”¨é™æ€æ±‡ç‡è¿›è¡Œè½¬æ¢ã€‚
   - **æ”¯æŒè´§å¸å¯¹**:
     - `TWD -> USD` (ä¾‹å¦‚: TSM å°ç§¯ç”µ)
     - `CNY -> USD` (ä¾‹å¦‚: BABA é˜¿é‡Œå·´å·´)
     - `HKD -> USD` (ä¾‹å¦‚: é¦™æ¸¯å…¬å¸ ADR)
     - `JPY -> USD` (ä¾‹å¦‚: æ—¥æœ¬å…¬å¸ ADR)
   - *æ³¨æ„*: ç›®å‰ä½¿ç”¨é™æ€æ±‡ç‡è¡¨ï¼Œé•¿æœŸå›æµ‹å¯èƒ½å­˜åœ¨ç²¾åº¦åå·®ã€‚

IV. Generic Derivation Engine (é€šç”¨æ¨å¯¼å¼•æ“)
----------------------------------------
æä¾›åº•å±‚çš„æ•°å­¦å·¥å…·ï¼Œç”¨äºåœ¨æ•°æ®ç¨€ç–æˆ–ç¼ºå¤±æ—¶å¡«è¡¥ä¼°å€¼ç©ºç™½ã€‚

1. **PE TTM æ¨å¯¼ç®—æ³•**:
   - `Daily PE = Daily Close / Latest EPS (FFill)`
   - æ™ºèƒ½è¯†åˆ«è´¢æŠ¥é¢‘åº¦ (Annual vs Quarterly)ï¼Œè‡ªåŠ¨æ‰§è¡Œ Rolling Sum (4 quarters) è®¡ç®— TTM EPSã€‚

2. **ç¨€ç–ç‚¹æ’å€¼ç®—æ³•**:
   - ç”¨äºå¤„ç†ä»…æœ‰å¹´åº¦æ•°æ®çš„æƒ…å†µã€‚
   - ç®—æ³•: å…ˆåæ¨ Implied EPSï¼Œçº¿æ€§æ’å€¼ç”Ÿæˆçš„è¿ç»­ EPS æ›²çº¿ï¼Œå†é™¤ä»¥æ¯æ—¥è‚¡ä»·å¾—åˆ°å¹³æ»‘ PEã€‚

V. Interactive CLI & Configuration
----------------------------------------
å†…ç½®å®Œæ•´çš„ç»ˆç«¯äº¤äº’ç•Œé¢ (`Config`, `print_menu`)ï¼ŒåŒæ—¶æ”¯æŒ Headless æ¨¡å¼ã€‚

1. **äº¤äº’æ¨¡å¼ (Default)**:
   - è¿è¡Œ `python fetch_valuation_history.py` è¿›å…¥èœå•ã€‚
   - æ”¯æŒæŒ‰å¸‚åœº (CN/HK/US) æ‰¹é‡ç­›é€‰ä¸‹è½½ã€‚

2. **Headless æ¨¡å¼ (Single Asset)**:
   - æ”¯æŒ `--symbol` å‚æ•°ç›´æ¥æŒ‡å®šèµ„äº§ IDï¼Œè·³è¿‡èœå• (e.g. `--symbol HK:STOCK:00700`)ã€‚
   - é€‚ç”¨äºè°ƒè¯•æˆ–è¢«å…¶ä»–è„šæœ¬è°ƒç”¨ã€‚

VI. Operational Details (è¿è¡Œç»†èŠ‚)
----------------------------------------
- **Snapshot Hot-Update (æ¸¯è‚¡)**: 
  Futu é€»è¾‘åŒ…å«"çƒ­æ›´æ–°"æœºåˆ¶ã€‚åœ¨ä¸‹è½½å†å²æ•°æ®çš„åŒæ—¶ï¼Œä¼šæ‹‰å–æœ€æ–°çš„å³æ—¶å¿«ç…§ (Snapshot) å¹¶è¦†ç›–å½“æ—¥çš„ PE TTMï¼Œç¡®ä¿ç›˜ä¸­æ•°æ®çš„å®æ—¶æ€§ã€‚
- **API Throttling**: 
  å†…ç½® `sleep(0.5)` é˜²æ­¢è§¦å‘å¤–éƒ¨ API (YFinance/FMP) çš„é€Ÿç‡é™åˆ¶ã€‚
- **History Limits**: 
  ç¾è‚¡ FMP æ¥å£é»˜è®¤è·å–æœ€è¿‘ 20 å¹´ (`limit=20`) çš„å¹´åº¦æ•°æ®ï¼Œä»¥ä¼˜åŒ–æ€§èƒ½ã€‚

å‰ç½®æ¡ä»¶:
1. **Futu OpenD**: å¿…é¡»åœ¨æœ¬åœ° 127.0.0.1:11111 è¿è¡Œå¹¶ç™»å½• (é’ˆå¯¹æ¸¯è‚¡)ã€‚
2. **FMP API Key**: éœ€é…ç½®æœ‰æ•ˆçš„ Financial Modeling Prep Key (é’ˆå¯¹ç¾è‚¡å†å²)ã€‚
3. **Database**: `MarketDataDaily` è¡¨éœ€é¢„å…ˆå¡«å…… OHLCV ä»·æ ¼æ•°æ®ã€‚


ä½œè€…: Antigravity
æ—¥æœŸ: 2026-01-23
"""
import sys
sys.path.append('backend')

import akshare as ak
import pandas as pd
import requests
import json
import argparse
import numpy as np
from datetime import datetime, timedelta
from sqlmodel import Session, select
from backend.database import engine
from backend.models import Watchlist, MarketDataDaily
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("FetchValuation")


def fetch_cn_valuation_history(symbol: str, asset_type: str = 'STOCK') -> pd.DataFrame:
    """
    è·å–Aè‚¡å†å²ä¼°å€¼æ•°æ®
    ä½¿ç”¨ AkShare stock_value_em() æ¥å£
    
    å‚æ•°:
        symbol: Canonical ID (å¦‚ CN:STOCK:600519)
        asset_type: èµ„äº§ç±»å‹ (STOCK, INDEX)
    """
    try:
        # 1. æ£€æŸ¥ç±»å‹ã€‚æŒ‡æ•°æš‚ä¸æ”¯æŒé€šè¿‡æ­¤æ¥å£è·å–ä¼°å€¼
        if asset_type == 'INDEX':
            logger.info(f"  â­ï¸  æŒ‡æ•°æš‚æ— ä¸ªè‚¡å¼ä¼°å€¼æ¥å£ï¼Œè·³è¿‡: {symbol}")
            return None
            
        # ä» Canonical ID æå–çº¯ä»£ç  (CN:STOCK:600519 -> 600519)
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        logger.info(f"  ğŸ“¥ è·å–Aè‚¡ä¼°å€¼æ•°æ®: {code}")
        
        # è°ƒç”¨ AkShare æ¥å£ (ä»…æ”¯æŒä¸ªè‚¡)
        df = ak.stock_value_em(symbol=code)
        
        if df is None or df.empty:
            logger.warning(f"  âš ï¸  æ— ä¼°å€¼æ•°æ®: {code}")
            return None
        
        logger.info(f"  âœ… è·å– {len(df)} æ¡ä¼°å€¼è®°å½•")
        return df
        
    except Exception as e:
        logger.error(f"  âŒ è·å–Aè‚¡ä¼°å€¼æ•°æ®å¤±è´¥ {symbol}: {e}")
        return None


def fetch_hk_valuation_baidu_direct(code: str, indicator: str = "å¸‚ç›ˆç‡(TTM)") -> pd.DataFrame:
    """
    ç›´æ¥è°ƒç”¨ç™¾åº¦è‚¡å¸‚é€š OpenData æ¥å£è·å–æ¸¯è‚¡å†å²ä¼°å€¼æ•°æ®
    
    å‚æ•°:
        code: 5ä½æ¸¯è‚¡ä»£ç  (å¦‚ '00700')
        indicator: 'å¸‚ç›ˆç‡(TTM)' æˆ– 'å¸‚å‡€ç‡'
    """
    try:
        url = "https://gushitong.baidu.com/opendata"
        params = {
            "openapi": "1",
            "dspName": "iphone",
            "tn": "tangram",
            "client": "app",
            "query": indicator,
            "code": code,
            "resource_id": "51171",
            "srcid": "51171",
            "market": "hk",
            "tag": indicator,
            "skip_industry": "1",
            "chart_select": "å…¨éƒ¨",
            "finClientType": "pc"
        }
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        logger.info(f"  ğŸŒ è°ƒç”¨ç™¾åº¦æ¥å£è·å– {indicator}: {code}")
        response = requests.get(url, params=params, headers=headers, timeout=10)
        
        if response.status_code != 200:
            logger.error(f"  âŒ æ¥å£å“åº”é”™è¯¯: {response.status_code}")
            return None
            
        data = response.json()
        
        # å¤æ‚ JSON è·¯å¾„æå–
        # Result[0].DisplayData.resultData.tplData.result.chartInfo[0].body
        try:
            results = data.get("Result", [])
            if not results:
                return None
            
            display_data = results[0].get("DisplayData", {}).get("resultData", {}).get("tplData", {}).get("result", {})
            chart_info = display_data.get("chartInfo", [])
            
            if not chart_info:
                return None
                
            body = chart_info[0].get("body", [])
            
            if not body:
                return None
            
            # body æ ¼å¼ä¸º [[date, value], ...]
            df = pd.DataFrame(body, columns=['date', 'value'])
            
            # è½¬æ¢æ—¥æœŸå’Œæ•°å€¼
            df['date'] = pd.to_datetime(df['date'])
            df['value'] = pd.to_numeric(df['value'], errors='coerce')
            
            return df
            
        except (KeyError, IndexError, TypeError) as e:
            logger.error(f"  âŒ è§£æç»“æœå¤±è´¥: {e}")
            return None
            
    except Exception as e:
        logger.error(f"  âŒ è°ƒç”¨ç™¾åº¦æ¥å£å¼‚å¸¸: {e}")
        return None


def fetch_cn_dividend_yield(symbol: str) -> float:
    """
    è·å–Aè‚¡æœ€æ–°è‚¡æ¯ç‡
    è®¡ç®—æ–¹å¼: Sum(æœ€è¿‘ä¸€å¹´æ¯è‚¡åˆ†çº¢) / å½“å‰è‚¡ä»·
    ä½¿ç”¨ AkShare: stock_fhps_detail_em (åˆ†çº¢) + stock_zh_a_hist (è‚¡ä»·)
    """
    try:
        import akshare as ak
        from datetime import datetime, timedelta
        
        # ä» Canonical ID æå–çº¯ä»£ç  (CN:STOCK:600030 -> 600030)
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        logger.info(f"  ğŸ“Š è·å–Aè‚¡è‚¡æ¯ç‡: {code}")
        
        # 1. è·å–å½“å‰è‚¡ä»·
        try:
            # è·å–æœ€è¿‘å‡ å¤©çš„Kçº¿ï¼Œå–æœ€æ–°æ”¶ç›˜ä»·
            # ä½¿ç”¨ qfq (å‰å¤æƒ) æ¯”è¾ƒåˆé€‚? ä¸ï¼Œè‚¡æ¯ç‡é€šå¸¸ç”¨ä¸å¤æƒä»·æ ¼è®¡ç®—å®æ—¶çš„ã€‚
            # ç›´æ¥å–æœ€è¿‘ä¸€æ¡è®°å½•
            start_dt = (datetime.now() - timedelta(days=10)).strftime("%Y%m%d")
            price_df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=start_dt, adjust="qfq")
            if price_df is None or price_df.empty:
                logger.warning(f"  âš ï¸  æ— æ³•è·å–è‚¡ä»·: {code}")
                return None
            current_price = float(price_df.iloc[-1]['æ”¶ç›˜'])
        except Exception as e:
            logger.warning(f"  âš ï¸  è·å–è‚¡ä»·å¤±è´¥: {e}")
            return None
            
        if current_price <= 0:
            return None

        # 2. è·å–åˆ†çº¢é…é€è¯¦æƒ…
        df = ak.stock_fhps_detail_em(symbol=code)
        
        if df is None or df.empty:
            return None
        
        if 'ç°é‡‘åˆ†çº¢-ç°é‡‘åˆ†çº¢æ¯”ä¾‹' not in df.columns:
            logger.warning("  âš ï¸  æ‰¾ä¸åˆ°åˆ†çº¢æ¯”ä¾‹åˆ—")
            return None

        # 3. ä½¿ç”¨"æŠ¥å‘ŠæœŸ"è®¡ç®—TTM (æœ€è¿‘ä¸€å¹´å®£å‘Šåˆ†çº¢)
        # é¿å…å› é™¤æƒé™¤æ¯æ—¥å˜åŠ¨å¯¼è‡´ "åˆšè¿‡365å¤©å°±å½’é›¶" çš„æƒ…å†µ
        report_col = 'æŠ¥å‘ŠæœŸ'
        if report_col not in df.columns:
             # å›é€€åˆ°æ—§é€»è¾‘ (é™¤æƒæ—¥)
             logger.warning("  âš ï¸  æ‰¾ä¸åˆ°æŠ¥å‘ŠæœŸåˆ—, å›é€€åˆ°é™¤æƒæ—¥é€»è¾‘")
             
             # ... (Fallback if needed, but for now we trust Report Date usually exists)
             date_col = 'é™¤æƒé™¤æ¯æ—¥'
             if date_col not in df.columns: return None
             df[date_col] = pd.to_datetime(df[date_col], errors='coerce').dt.date
             recent_dividends = df[df[date_col] >= (datetime.now().date() - timedelta(days=365))]
        else:
            df[report_col] = pd.to_datetime(df[report_col], errors='coerce').dt.date
            max_report_date = df[report_col].max()
            
            if pd.isna(max_report_date):
                return 0.0
                
            # æˆªæ–­æ—¥æœŸ: æœ€æ–°æŠ¥å‘ŠæœŸ - 1å¹´
            # ä¾‹å¦‚: æœ€æ–° 2024-12-31. æˆªæ–­ 2023-12-31.
            # æˆ‘ä»¬éœ€è¦ > 2023-12-31 çš„è®°å½• (å³ 2024-06, 2024-12).
            cutoff_date = max_report_date - timedelta(days=365)
            
            recent_dividends = df[df[report_col] > cutoff_date]
        
        if recent_dividends.empty:
            logger.info(f"  â„¹ï¸  è¿‡å»ä¸€å¹´æ— åˆ†çº¢å®£å‘Š (Based on Report Date)")
            return 0.0
            
        # 4. è®¡ç®—æ€»æ¯è‚¡åˆ†çº¢ (DPS)
        # åˆ—å: 'ç°é‡‘åˆ†çº¢-ç°é‡‘åˆ†çº¢æ¯”ä¾‹' (æ¯10è‚¡æ´¾å¤šå°‘å…ƒ)
        sum_per_10 = recent_dividends['ç°é‡‘åˆ†çº¢-ç°é‡‘åˆ†çº¢æ¯”ä¾‹'].sum()
        total_dps = sum_per_10 / 10.0

        # 5. è®¡ç®—è‚¡æ¯ç‡
        dividend_yield = (total_dps / current_price) * 100
        
        logger.info(f"  âœ… TTMè‚¡æ¯ç‡(å®£å‘Š): {dividend_yield:.2f}% (DPS: {total_dps}, Price: {current_price})")
        return dividend_yield
        
    except Exception as e:
        logger.warning(f"  âš ï¸  è·å–Aè‚¡è‚¡æ¯ç‡å¤±è´¥ {symbol}: {e}")
        return None


def fetch_hk_dividend_yield(symbol: str) -> float:
    """
    è·å–æ¸¯è‚¡æœ€æ–°è‚¡æ¯ç‡
    ä½¿ç”¨yfinance
    """
    try:
        import yfinance as yf
        # ä» Canonical ID æå–çº¯ä»£ç  (HK:STOCK:00700 -> 00700)
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        # è½¬æ¢ä¸ºyfinanceæ ¼å¼ (00700 -> 0700.HK)
        # yfinance éœ€è¦4ä½æ•°å­—ä»£ç  (ä¾‹å¦‚ 0700.HK)
        clean_code = code.lstrip('0')
        if len(clean_code) < 4:
            clean_code = clean_code.zfill(4)
        yf_symbol = f"{clean_code}.HK"
        
        logger.info(f"  ğŸ“Š è·å–æ¸¯è‚¡è‚¡æ¯ç‡: {yf_symbol}")
        
        ticker = yf.Ticker(yf_symbol)
        
        # ç›´æ¥ä» info è·å– (ç”¨æˆ·å€¾å‘äº Direct Fetch)
        info = ticker.info
        dividend_yield = info.get('dividendYield')
        
        if dividend_yield is not None:
             # yfinance è¿”å›çš„å°±æ˜¯ç™¾åˆ†æ¯”æ•°å€¼ (e.g. 4.0 = 4%)
             # æ— éœ€ä¹˜ä»¥100
             converted_yield = dividend_yield
             logger.info(f"  âœ… [Fetch] æ¸¯è‚¡è‚¡æ¯ç‡: {converted_yield:.2f}%")
             return converted_yield
        
        logger.warning(f"  âš ï¸  æ— æ³•è·å–æ¸¯è‚¡è‚¡æ¯ç‡ (Info is None)")
        return None
        
    except Exception as e:
        logger.warning(f"  âš ï¸  è·å–æ¸¯è‚¡è‚¡æ¯ç‡å¤±è´¥ {symbol}: {e}")
        return None


def fetch_hk_valuation_history(symbol: str, indicator: str = "å¸‚ç›ˆç‡") -> pd.DataFrame:
    """
    è·å–æ¸¯è‚¡å†å²ä¼°å€¼æ•°æ®
    ä½¿ç”¨ç™¾åº¦æ¥å£ (TTM PE å’Œ PB)
    """
    try:
        # ä» Canonical ID æå–çº¯ä»£ç  (HK:STOCK:00700 -> 00700)
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        if indicator == "å¸‚ç›ˆç‡(TTM)":
            # è·å– TTM PE
            df = fetch_hk_valuation_baidu_direct(code, indicator="å¸‚ç›ˆç‡(TTM)")
            if df is not None and not df.empty:
                df = df.rename(columns={'value': 'pe'}) # Temporary rename, value is generic
                return df
        elif indicator == "å¸‚ç›ˆç‡":
             # è·å– Static PE (Baidu usually 'å¸‚ç›ˆç‡' implies Static/Lyr or just PE)
             df = fetch_hk_valuation_baidu_direct(code, indicator="å¸‚ç›ˆç‡")
             if df is not None and not df.empty:
                 df = df.rename(columns={'value': 'pe'})
                 return df
                
        elif indicator == "å¸‚å‡€ç‡":
            # è·å– PB
            df = fetch_hk_valuation_baidu_direct(code, indicator="å¸‚å‡€ç‡")
            if df is not None and not df.empty:
                df = df.rename(columns={'value': 'pb'})
                logger.info(f"  âœ… è·å– {len(df)} æ¡ PB è®°å½•")
                return df
        
        return None
            
    except Exception as e:
        logger.error(f"  âŒ è·å–æ¸¯è‚¡{indicator}æ•°æ®å¤±è´¥ {symbol}: {e}")
        return None


def save_cn_valuation_to_daily(symbol: str, df: pd.DataFrame, session: Session) -> int:
    """
    å°†Aè‚¡ä¼°å€¼æ•°æ®ä¿å­˜åˆ° MarketDataDaily è¡¨
    æ›´æ–° pe_ratio å’Œ pb_ratio å­—æ®µ
    """
    if df is None or df.empty:
        return 0
    
    updated_count = 0
    
    for _, row in df.iterrows():
        try:
            # è§£ææ—¥æœŸ
            date_str = str(row['æ•°æ®æ—¥æœŸ'])
            if len(date_str) == 8:  # YYYYMMDD
                timestamp_str = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]} 15:00:00"
            else:
                # å·²ç»æ˜¯ YYYY-MM-DD æ ¼å¼
                timestamp_str = f"{date_str} 15:00:00"
            
            # æŸ¥æ‰¾å¯¹åº”çš„æ—¥çº¿è®°å½•
            existing = session.exec(
                select(MarketDataDaily).where(
                    MarketDataDaily.symbol == symbol,
                    MarketDataDaily.timestamp == timestamp_str
                )
            ).first()

            # Fallback: å°è¯• 00:00:00 (å¦‚æœETLæœªå½’ä¸€åŒ–)
            if not existing:
                if len(date_str) == 8:
                    timestamp_00 = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]} 00:00:00"
                else:
                    timestamp_00 = f"{date_str} 00:00:00"
                
                existing = session.exec(
                    select(MarketDataDaily).where(
                        MarketDataDaily.symbol == symbol,
                        MarketDataDaily.timestamp == timestamp_00
                    )
                ).first()
                if existing:
                    # Self-heal timestamp
                    existing.timestamp = timestamp_str
            
            if existing:
                # æ›´æ–°PEå’ŒPB
                existing.pe_ttm = float(row['PE(TTM)']) if pd.notna(row['PE(TTM)']) else None
                # PE(é™) mapping
                existing.pe = float(row['PE(é™)']) if 'PE(é™)' in row and pd.notna(row['PE(é™)']) else None
                
                existing.pb = float(row['å¸‚å‡€ç‡']) if pd.notna(row['å¸‚å‡€ç‡']) else None
                # ä¿å­˜è‚¡æ¯ç‡(å¦‚æœæœ‰)
                if 'dividend_yield' in row and pd.notna(row['dividend_yield']):
                    existing.dividend_yield = float(row['dividend_yield'])
                existing.updated_at = datetime.now()
                session.add(existing)
                updated_count += 1
                
        except Exception as e:
            logger.warning(f"  âš ï¸  è·³è¿‡è®°å½• {date_str}: {e}")
            continue
    
    if updated_count > 0:
        session.commit()
        logger.info(f"  ğŸ’¾ æ›´æ–° {updated_count} æ¡è®°å½•çš„PE/PBæ•°æ®")
    
    return updated_count


def save_hk_valuation_to_daily(symbol: str, df_pe_ttm: pd.DataFrame, df_pe_static: pd.DataFrame, df_pb: pd.DataFrame, session: Session) -> int:
    """
    å°†æ¸¯è‚¡ä¼°å€¼æ•°æ®ä¿å­˜åˆ° MarketDataDaily è¡¨
    æ›´æ–° pe_ratio å’Œ pb_ratio å­—æ®µ
    
    æ³¨æ„: ç™¾åº¦è¿”å›çš„æ—¥æœŸå¯èƒ½ä¸å®é™…äº¤æ˜“æ—¥æœŸæœ‰åå·®,ä½¿ç”¨æ—¥æœŸéƒ¨åˆ†åŒ¹é…
    """
    updated_count = 0
    pe_ttm_matched = 0
    pe_static_matched = 0
    pb_matched = 0
    
    # å¤„ç† PE TTM
    if df_pe_ttm is not None and not df_pe_ttm.empty:
        logger.info(f"  ğŸ“Š å¤„ç† {len(df_pe_ttm)} æ¡ PE(TTM) æ•°æ®...")
        for _, row in df_pe_ttm.iterrows():
            try:
                date = pd.to_datetime(row['date'])
                date_str = date.strftime('%Y-%m-%d')
                val = float(row['pe']) if pd.notna(row['pe']) else None
                if val is None: continue

                # Match logic: Try nearest within +/- 5 days
                matched = False
                offsets = [0, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5]
                for offset in offsets:
                    target_date = date + pd.Timedelta(days=offset)
                    timestamp_str = target_date.strftime('%Y-%m-%d') + ' 16:00:00'
                    existing = session.exec(select(MarketDataDaily).where(MarketDataDaily.symbol == symbol, MarketDataDaily.market == 'HK', MarketDataDaily.timestamp == timestamp_str)).first()
                    if not existing:
                        timestamp_00 = target_date.strftime('%Y-%m-%d') + ' 00:00:00'
                        existing = session.exec(select(MarketDataDaily).where(MarketDataDaily.symbol == symbol, MarketDataDaily.market == 'HK', MarketDataDaily.timestamp == timestamp_00)).first()
                        if existing: existing.timestamp = timestamp_str
                    
                    if existing:
                        existing.pe_ttm = val
                        existing.updated_at = datetime.now()
                        session.add(existing)
                        pe_ttm_matched += 1
                        matched = True
                        break
            except Exception: continue

    # å¤„ç† PE Static
    if df_pe_static is not None and not df_pe_static.empty:
        logger.info(f"  ğŸ“Š å¤„ç† {len(df_pe_static)} æ¡ PE(Static) æ•°æ®...")
        for _, row in df_pe_static.iterrows():
            try:
                date = pd.to_datetime(row['date'])
                val = float(row['pe']) if pd.notna(row['pe']) else None
                if val is None: continue
                
                matched = False
                offsets = [0, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5]
                for offset in offsets:
                    target_date = date + pd.Timedelta(days=offset)
                    timestamp_str = target_date.strftime('%Y-%m-%d') + ' 16:00:00'
                    existing = session.exec(select(MarketDataDaily).where(MarketDataDaily.symbol == symbol, MarketDataDaily.market == 'HK', MarketDataDaily.timestamp == timestamp_str)).first()
                    if not existing:
                        timestamp_00 = target_date.strftime('%Y-%m-%d') + ' 00:00:00'
                        existing = session.exec(select(MarketDataDaily).where(MarketDataDaily.symbol == symbol, MarketDataDaily.market == 'HK', MarketDataDaily.timestamp == timestamp_00)).first()
                        if existing: existing.timestamp = timestamp_str
                    
                    if existing:
                        existing.pe = val
                        existing.updated_at = datetime.now()
                        session.add(existing)
                        pe_static_matched += 1
                        matched = True
                        break
            except Exception: continue
    
    # å¤„ç†PBæ•°æ®
    if df_pb is not None and not df_pb.empty:
        logger.info(f"  ğŸ“Š å¤„ç† {len(df_pb)} æ¡PBæ•°æ®...")
        for _, row in df_pb.iterrows():
            try:
                date = pd.to_datetime(row['date'])
                date_str = date.strftime('%Y-%m-%d')
                pb_value = float(row['pb']) if pd.notna(row['pb']) else None
                
                if pb_value is None:
                    continue
                
                # Match logic: Try nearest within +/- 5 days
                matched = False
                # Priority: 0, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5
                offsets = [0, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5]
                
                for offset in offsets:
                    target_date = date + pd.Timedelta(days=offset)
                    timestamp_str = target_date.strftime('%Y-%m-%d') + ' 16:00:00'
                    existing = session.exec(select(MarketDataDaily).where(MarketDataDaily.symbol == symbol, MarketDataDaily.market == 'HK', MarketDataDaily.timestamp == timestamp_str)).first()
                    
                    if not existing:
                        timestamp_00 = target_date.strftime('%Y-%m-%d') + ' 00:00:00'
                        existing = session.exec(select(MarketDataDaily).where(MarketDataDaily.symbol == symbol, MarketDataDaily.market == 'HK', MarketDataDaily.timestamp == timestamp_00)).first()
                        if existing: existing.timestamp = timestamp_str # Self-heal
                    
                    if existing:
                        # Avoid overwriting if we already have a closer match? 
                        # Ideally updates are idempotent.
                        existing.pb = pb_value
                        existing.updated_at = datetime.now()
                        session.add(existing)
                        pb_matched += 1
                        matched = True
                        break
                
                if not matched:
                    logger.debug(f"  âš ï¸  æœªåŒ¹é…åˆ°PBè®°å½•: {date_str}")
                    
            except Exception as e:
                logger.warning(f"  âŒ å¤„ç†PBè®°å½•å¤±è´¥ {date_str}: {e}")
                continue
    
    updated_count = pe_ttm_matched + pe_static_matched + pb_matched
    
    if updated_count > 0:
        session.commit()
        logger.info(f"  ğŸ’¾ æ›´æ–° {updated_count} æ¡è®°å½• (PE_TTM: {pe_ttm_matched}, PE_Static: {pe_static_matched}, PB: {pb_matched})")
    else:
        logger.warning(f"  âš ï¸  æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•è®°å½•")
    
    return updated_count


def fetch_us_valuation_yfinance(symbol: str) -> dict:
    """
    ä» yfinance è·å–ç¾è‚¡çš„å®æ—¶ä¼°å€¼æŒ‡æ ‡
    ä½¿ç”¨ ticker.info API
    """
    try:
        import yfinance as yf
        
        # ä» Canonical ID æå–çº¯ä»£ç  (US:STOCK:AAPL -> AAPL)
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        logger.info(f"  ğŸ“¥ è·å–ç¾è‚¡ä¼°å€¼æ•°æ®: {code}")
        
        ticker = yf.Ticker(code)
        info = ticker.info
        
        # è·å– PE (ä¼˜å…ˆä½¿ç”¨ trailingPE, å…¶æ¬¡ forwardPE)
        pe = info.get('trailingPE') or info.get('forwardPE')
        
        # è·å– PB
        pb = info.get('priceToBook')
        
        # è·å–è‚¡æ¯ç‡
        dividend_yield = info.get('dividendYield')
        # æ— éœ€ä¹˜ä»¥100
        if dividend_yield is not None:
            pass
        
        result = {
            'pe_ttm': pe, # Yfinance trailingPE -> pe_ttm
            'pb': pb,
            'dividend_yield': dividend_yield
        }
        
        # æ ¼å¼åŒ–è¾“å‡º
        pe_str = f"{pe:.2f}" if pe else "N/A"
        pb_str = f"{pb:.2f}" if pb else "N/A"
        div_str = f"{dividend_yield:.2f}%" if dividend_yield else "N/A"
        logger.info(f"  âœ… PE: {pe_str}, PB: {pb_str}, è‚¡æ¯ç‡: {div_str}")
        
        return result
        
    except Exception as e:
        logger.error(f"  âŒ è·å–ç¾è‚¡ä¼°å€¼æ•°æ®å¤±è´¥ {symbol}: {e}")
        return None



FMP_API_KEY = "yytaAKONtPbR5cBcx9azLeqlovaWDRQm"

def fetch_us_valuation_history_fmp(symbol: str, limit: int = 5) -> pd.DataFrame:
    """
    ä» FMP Cloud è·å–ç¾è‚¡å†å²ä¼°å€¼æ•°æ® (PE, PB)
    ä½¿ç”¨ /stable/ratios æ¥å£
    """
    try:
        # çº¯ä»£ç 
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        url = f"https://financialmodelingprep.com/stable/ratios?symbol={code}&period=annual&limit={limit}&apikey={FMP_API_KEY}"
        logger.info(f"  ğŸ“¥ [FMP] è·å–ç¾è‚¡å†å²ä¼°å€¼: {code}")
        
        response = requests.get(url, timeout=10)
        data = response.json()
        
        if not data:
            logger.info(f"  â„¹ï¸  [FMP] æ— å†å²æ•°æ®: {code}")
            return None
        
        # å¤„ç† API é”™è¯¯ (ä¾‹å¦‚ Limit Reach)
        if isinstance(data, dict) and "Error Message" in data:
             logger.warning(f"  âš ï¸ [FMP] API é™åˆ¶æˆ–é”™è¯¯: {data['Error Message']}")
             return None

        # è½¬æ¢ä¸º DataFrame
        records = []
        for item in data:
            if not isinstance(item, dict): continue
            records.append({
                'date': item.get('date'),
                'pe': item.get('priceToEarningsRatio'),
                'pb': item.get('priceToBookRatio')
            })
            
        df = pd.DataFrame(records)
        logger.info(f"  âœ… [FMP] è·å– {len(df)} æ¡å†å²è®°å½•")
        return df
        
    except Exception as e:
        logger.error(f"  âŒ [FMP] è·å–å¤±è´¥ {symbol}: {e}")
        return None

def fetch_us_valuation_history_fmp_ttm(symbol: str, limit: int = 365) -> pd.DataFrame:
    """
    ä» FMP Cloud è·å–ç¾è‚¡æ¯æ—¥æ»šåŠ¨ PE (TTM)
    ä½¿ç”¨ /v3/ratios-ttm æ¥å£
    """
    try:
        # çº¯ä»£ç 
        code = symbol.split(':')[-1] if ':' in symbol else symbol
        
        # ä½¿ç”¨ v3 æ¥å£ (æ ¹æ®ç”¨æˆ·è¯·æ±‚)
        url = f"https://financialmodelingprep.com/api/v3/ratios-ttm/{code}?limit={limit}&apikey={FMP_API_KEY}"
        logger.info(f"  ğŸ“¥ [FMP] è·å–ç¾è‚¡ TTM PE: {code}")
        
        response = requests.get(url, timeout=10)
        
        # é”™è¯¯å¤„ç†
        if response.status_code == 403:
             logger.warning(f"  âš ï¸ [FMP] API Key æ— æ•ˆæˆ–æ—  TTM æƒé™ (403): {response.text[:100]}")
             return None
             
        data = response.json()
        
        if not data:
            logger.info(f"  â„¹ï¸  [FMP] æ—  TTM æ•°æ®: {code}")
            return None
        
        if isinstance(data, dict) and "Error Message" in data:
             logger.warning(f"  âš ï¸ [FMP] API é”™è¯¯: {data['Error Message']}")
             return None

        # è½¬æ¢ä¸º DataFrame
        records = []
        for item in data:
            if not isinstance(item, dict): continue
            
            # FMP TTM field: peRatioTTM
            val = item.get('peRatioTTM')
            if val is None: val = item.get('priceEarningsRatioTTM')
            
            # --- VERA Pro Fields ---
            # NOTE: The endpoint /ratios-ttm usually only returns Ratios (PE, PB, etc.), NOT fundamentals.
            # To get NetIncomeCommon and SharesDiluted(TTM), we strictly need /key-metrics-ttm or calculate from /income-statement.
            # However, for simple PE history backfill (this function's purpose is PE history), we might just be saving PE TTM directly?
            # Wait, `fetch_us_valuation_history_fmp_ttm` returns a DF that is saving to MarketDataDaily directly in `recalc_historical_pe` or saving to Financials?
            # Looking at `save_us_historical_valuation_to_daily`, it saves directly to DAILY.
            # This function is for "downloading pre-calculated PE from FMP".
            
            # BUT, the user wants us to CALCULATE locally using Fundamentals.
            # So we need a NEW function in `fetch_financials.py` (or here) that fetches INCOME STATEMENT and fills FinancialFundamentals.
            # This function `fetch_us_valuation_history_fmp_ttm` is about fetching PE directly.
            
            # Let's keep this as is for BACKUP PE sources, but we need to ensure we can FETCH FUNDAMENTALS.
            # The user instruction was: "Update FMP API calls to fetch specific fields: netIncomeForCommonStockholders..."
            # This implies we need to update where we fetch FinancialFundamentals.
            
            if val is None: continue
            
            records.append({
                'date': item.get('date'),
                'pe_ttm': val
            })
            
        df = pd.DataFrame(records)
        logger.info(f"  âœ… [FMP] è·å– {len(df)} æ¡ TTM è®°å½•")
        return df
        
    except Exception as e:
        logger.error(f"  âŒ [FMP] è·å– TTM å¤±è´¥ {symbol}: {e}")
        return None

def save_us_historical_valuation_to_daily(symbol: str, df: pd.DataFrame, session: Session) -> int:
    """
    ä¿å­˜ FMP å†å²ä¼°å€¼æ•°æ®åˆ° MarketDataDaily (Fallback logic)
    æ”¯æŒ pe, pb, pe_ttm
    """
    if df is None or df.empty:
        return 0
        
    updated_count = 0
    
    for _, row in df.iterrows():
        try:
            date_str = row['date'] # YYYY-MM-DD
            pe = float(row['pe']) if 'pe' in row and pd.notna(row['pe']) else None
            pe_ttm = float(row['pe_ttm']) if 'pe_ttm' in row and pd.notna(row['pe_ttm']) else None
            pb = float(row['pb']) if 'pb' in row and pd.notna(row['pb']) else None
            
            if pe is None and pb is None and pe_ttm is None:
                continue
                
            task_date = datetime.strptime(date_str, "%Y-%m-%d").date()
            
            matched = False
            for offset in [0, -1, -2, -3]:
                d = task_date + timedelta(days=offset)
                ts_str = d.strftime("%Y-%m-%d") + " 16:00:00"
                
                existing = session.exec(
                    select(MarketDataDaily).where(
                        MarketDataDaily.symbol == symbol,
                        MarketDataDaily.market == 'US',
                        MarketDataDaily.timestamp == ts_str
                    )
                ).first()
                
                # Fallback: å°è¯• 00:00
                if not existing:
                    ts_str_00 = d.strftime("%Y-%m-%d") + " 00:00:00"
                    existing = session.exec(
                        select(MarketDataDaily).where(
                            MarketDataDaily.symbol == symbol,
                            MarketDataDaily.market == 'US',
                            MarketDataDaily.timestamp == ts_str_00
                        )
                    ).first()
                    if existing:
                        existing.timestamp = ts_str

                if existing:
                    if pe is not None: existing.pe = pe
                    if pe_ttm is not None: existing.pe_ttm = pe_ttm
                    if pb is not None: existing.pb = pb
                    existing.updated_at = datetime.now()
                    session.add(existing)
                    updated_count += 1
                    matched = True
                    break
            
            if not matched:
                logger.debug(f"  âš ï¸  [FMP] æœªåŒ¹é…åˆ°å†å²æ—¥çº¿: {date_str}")

        except Exception as e:
            logger.warning(f"  âš ï¸  ä¿å­˜å¤±è´¥ {date_str}: {e}")
            
    if updated_count > 0:
        session.commit()
    return updated_count


def save_us_valuation_to_daily(symbol: str, valuation: dict, session: Session) -> int:
    """
    å°†ç¾è‚¡ä¼°å€¼æ•°æ®ä¿å­˜åˆ° MarketDataDaily è¡¨
    """
    if not valuation:
        return 0
    
    try:
        from backend.market_status import MarketStatus
        
        is_market_open = MarketStatus.is_market_open('US')
        market_time = MarketStatus.get_market_time('US')
        
        latest_record = session.exec(
            select(MarketDataDaily).where(
                MarketDataDaily.symbol == symbol,
                MarketDataDaily.market == 'US'
            ).order_by(MarketDataDaily.timestamp.desc())
        ).first()
        
        if not latest_record:
            logger.warning(f"  âš ï¸  æœªæ‰¾åˆ°æ—¥çº¿è®°å½•: {symbol}")
            return 0
        
        record_date = datetime.strptime(latest_record.timestamp, '%Y-%m-%d %H:%M:%S').date()
        today = market_time.date()
        
        if is_market_open and record_date == today:
            logger.info(f"  â­ï¸  ç›˜ä¸­æ—¶æ®µ,è·³è¿‡ä»Šæ—¥æ•°æ®,ä»…æ›´æ–°å‰ä¸€äº¤æ˜“æ—¥: {symbol}")
            records = session.exec(
                select(MarketDataDaily).where(
                    MarketDataDaily.symbol == symbol,
                    MarketDataDaily.market == 'US'
                ).order_by(MarketDataDaily.timestamp.desc())
            ).all()
            
            if len(records) < 2:
                logger.warning(f"  âš ï¸  æ²¡æœ‰å‰ä¸€äº¤æ˜“æ—¥è®°å½•: {symbol}")
                return 0
            latest_record = records[1]
        
        updated = False
        if valuation.get('pe_ttm'):
            latest_record.pe_ttm = valuation['pe_ttm']
            updated = True
        # Static PE not updated from Yfinance (Realtime) usually
        if valuation.get('pe'):
             latest_record.pe = valuation['pe']
             updated = True
        if valuation.get('pb'):
            latest_record.pb = valuation['pb']
            updated = True
        if valuation.get('dividend_yield'):
            latest_record.dividend_yield = valuation['dividend_yield']
            updated = True
        
        if updated:
            latest_record.updated_at = datetime.now()
            session.add(latest_record)
            session.commit()
            logger.info(f"  ğŸ’¾ æ›´æ–°è®°å½•çš„ä¼°å€¼æ•°æ® ({latest_record.timestamp})")
            return 1
        
        return 0
        
    except Exception as e:
        logger.error(f"  âŒ ä¿å­˜ç¾è‚¡ä¼°å€¼æ•°æ®å¤±è´¥ {symbol}: {e}")
        return 0


def derive_daily_pe_from_points(symbol: str, daily_prices: pd.DataFrame, report_points: pd.DataFrame) -> pd.Series:
    """
    é€šç”¨å‡½æ•°: åŸºäºç¨€ç–çš„ PE æŠ¥å‘Šç‚¹æ¨å¯¼æ—¥çº¿ PE
    """
    try:
        if daily_prices.empty or report_points.empty:
            return pd.Series(dtype=float)
            
        # 1. å‡†å¤‡æ•°æ®
        reports = report_points.sort_values('date').copy()
        reports = reports[reports['pe'] > 0]
        
        if reports.empty:
            return pd.Series(dtype=float)

        daily_idx = daily_prices.index.sort_values()
        
        # 2. è®¡ç®—æ¯ä¸ªæŠ¥å‘Šæ—¥çš„éšå« EPS
        eps_map = {}
        for _, row in reports.iterrows():
            r_date = row['date']
            r_pe = row['pe']
            
            # åœ¨æ—¥çº¿ä»·æ ¼ä¸­æ‰¾ exact match æˆ– nearest (Look-back only)
            matched_date = None
            try:
                # ä½¿ç”¨ asof å¯»æ‰¾æœ€è¿‘çš„ä¸€ä¸ªäº¤æ˜“æ—¥ (Look-back, é¿å…æœªæ¥æ•°æ®)
                # asof returns the last index label <= r_date
                closest_date = daily_idx.asof(r_date)
                
                if pd.isna(closest_date):
                    continue
                    
                # æ£€æŸ¥æ—¶é—´è·¨åº¦æ˜¯å¦è¿‡å¤§ (è¶…è¿‡ 5 å¤©è§†ä¸ºæ— æ•ˆåŒ¹é…)
                if (r_date - closest_date).days > 5:
                    continue
                    
                matched_date = closest_date
                close = daily_prices.loc[matched_date, 'close']

                if isinstance(close, pd.Series): close = close.iloc[0]
                
                if close > 0 and r_pe > 0:
                    eps_implied = close / r_pe
                    if matched_date:
                        eps_map[matched_date] = eps_implied
            except:
                continue
                
        if not eps_map:
            return pd.Series(dtype=float)
            
        # 3. ç”Ÿæˆ EPS åºåˆ—å¹¶å¯¹é½åˆ°æ—¥çº¿
        eps_series = pd.Series(eps_map).sort_index()
        eps_daily = eps_series.reindex(daily_idx).ffill()
        
        # 4. è®¡ç®— Daily PE
        daily_pe = daily_prices['close'] / eps_daily
        daily_pe = daily_pe.replace([np.inf, -np.inf], np.nan).dropna()
        
        return daily_pe.round(2)
        
    except Exception as e:
        logger.error(f"  âŒ æ¨å¯¼å¤±è´¥ {symbol}: {e}")
        return pd.Series(dtype=float)


def derive_daily_pb_from_points(symbol: str, daily_prices: pd.DataFrame, report_points: pd.DataFrame) -> pd.Series:
    """
    é€šç”¨å‡½æ•°: åŸºäºç¨€ç–çš„ PB æŠ¥å‘Šç‚¹æ¨å¯¼æ—¥çº¿ PB
    é€»è¾‘åŒ PE æ¨å¯¼: Implied BPS = Close / PB
    """
    try:
        if daily_prices.empty or report_points.empty:
            return pd.Series(dtype=float)
            
        # 1. å‡†å¤‡æ•°æ®
        reports = report_points.sort_values('date').copy()
        reports = reports[reports['pb'] > 0]
        
        if reports.empty:
            return pd.Series(dtype=float)

        daily_idx = daily_prices.index.sort_values()
        
        # 2. è®¡ç®—æ¯ä¸ªæŠ¥å‘Šæ—¥çš„éšå« BPS
        bps_map = {}
        for _, row in reports.iterrows():
            r_date = row['date']
            r_pb = row['pb']
            
            # åœ¨æ—¥çº¿ä»·æ ¼ä¸­æ‰¾ match (Look-back only)
            matched_date = None
            try:
                # ä½¿ç”¨ asof å¯»æ‰¾æœ€è¿‘çš„ä¸€ä¸ªäº¤æ˜“æ—¥ (Look-back, é¿å…æœªæ¥æ•°æ®)
                # asof returns the last index label <= r_date
                closest_date = daily_idx.asof(r_date)
                
                if pd.isna(closest_date):
                    continue
                    
                # æ£€æŸ¥æ—¶é—´è·¨åº¦æ˜¯å¦è¿‡å¤§
                if (r_date - closest_date).days > 5:
                    continue
                    
                matched_date = closest_date
                close = daily_prices.loc[matched_date, 'close']

                if isinstance(close, pd.Series): close = close.iloc[0]
                
                if close > 0 and r_pb > 0:
                    bps_implied = close / r_pb
                    if matched_date:
                        bps_map[matched_date] = bps_implied
            except:
                continue
                
        if not bps_map:
            return pd.Series(dtype=float)
            
        # 3. ç”Ÿæˆ BPS åºåˆ—å¹¶å¯¹é½åˆ°æ—¥çº¿
        bps_series = pd.Series(bps_map).sort_index()
        bps_daily = bps_series.reindex(daily_idx).ffill()
        
        # 4. è®¡ç®— Daily PB
        daily_pb = daily_prices['close'] / bps_daily
        daily_pb = daily_pb.replace([np.inf, -np.inf], np.nan).dropna()
        
        return daily_pb.round(2)
        
    except Exception as e:
        logger.error(f"  âŒ PBæ¨å¯¼å¤±è´¥ {symbol}: {e}")
        return pd.Series(dtype=float)


# ==============================================================================
# Local Derivation (Fallback for missing API data)
# ==============================================================================

def derive_pe_ttm_from_fundamentals(symbol: str, session: Session) -> pd.DataFrame:
    """
    3. **History (Derivation)**: **æœ¬åœ°æ¨å¯¼å¼•æ“** (Fallback)ã€‚
       - å½“ API ç¼ºå¤±æ—¶ï¼Œåˆ©ç”¨æœ¬åœ° `FinancialFundamentals` (EPS) + `MarketDataDaily` (Close) è®¡ç®—ã€‚
       - **é«˜çº§ç‰¹æ€§: æ±‡ç‡è‡ªåŠ¨å¯¹é½ (Currency Alignment)**
         æœ¬è„šæœ¬åŒ…å«ç¡¬ç¼–ç çš„è¿‘ä¼¼æ±‡ç‡é€»è¾‘ï¼Œä»¥è§£å†³è´¢æŠ¥è´§å¸ä¸äº¤æ˜“è´§å¸ä¸ä¸€è‡´çš„é—®é¢˜ï¼š
         - **US Market (ADR)**: TWD/CNY/HKD/JPY -> USD (e.g. TSM: TWD EPS -> USD Price).
         - **HK Market**: CNY/USD -> HKD (e.g. Hè‚¡è´¢æŠ¥ä¸º CNY -> æ¸¯è‚¡æŠ¥ä»·ä¸º HKD).
         - *æ³¨æ„*: ä½¿ç”¨é™æ€æ±‡ç‡ (Static Rates)ï¼Œå†å²ç²¾åº¦ä¸å¦‚ä¸“ä¸šç‰ˆåŠ¨æ€æ±‡ç‡ã€‚
       - **é¢‘åº¦å¤„ç†**: æ™ºèƒ½è¯†åˆ« Annual/Quarterly æŠ¥å‘Šå¹¶è¿›è¡Œ Rolling TTM è®¡ç®—ã€‚
    """
    try:
        from backend.models import FinancialFundamentals
        
        # 1. è·å–æ—¥çº¿ä»·æ ¼
        daily_prices = pd.read_sql(
            select(MarketDataDaily.timestamp, MarketDataDaily.close)
            .where(MarketDataDaily.symbol == symbol)
            .order_by(MarketDataDaily.timestamp),
            engine
        )
        if daily_prices.empty:
            return None
            
        daily_prices['timestamp'] = pd.to_datetime(daily_prices['timestamp'])
        daily_prices.set_index('timestamp', inplace=True)
        # Ensure numeric
        daily_prices['close'] = pd.to_numeric(daily_prices['close'], errors='coerce')
        
        # 2. è·å–è´¢åŠ¡æ•°æ® (EPS)
        # ä¼˜å…ˆä½¿ç”¨ eps å­—æ®µ (å‡è®¾ä¸º Basic/Diluted EPS)
        # TODO: åŒºåˆ† Annual/Quarterly å¹¶æ­£ç¡®è®¡ç®— TTM EPS.
        # ä½† FinancialFundamentals å­˜å‚¨çš„æ˜¯åŸå§‹ Report æ•°æ®.
        # å¦‚æœæ˜¯ Annual, EPS is annual. If Quarterly, EPS is quarterly.
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åªèƒ½å‡è®¾ fetch_financials å·²ç»å°½åŠ›è·å–äº† EPS.
        # å¦‚æœæ˜¯ç¾è‚¡ï¼Œfetch_financials å¯èƒ½è·å–çš„æ˜¯ Quarterly?
        # Aè‚¡ (AkShare) è·å–çš„æ˜¯ Annual/Quarterly.
        # æˆ‘ä»¬è¿™é‡Œæš‚æ—¶å‡è®¾: ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡æŠ¥å‘Šçš„ EPS * (å¦‚æœæ˜¯å­£åº¦åˆ™éœ€è°ƒæ•´? ä¸, PE TTM éœ€è¦ TTM EPS)
        # å¦‚æœ FinancialFundamentals é‡Œçš„æ˜¯ Quarterly EPS, å•å­£ EPS ä¸èƒ½ç›´æ¥ç”¨.
        # éœ€è¦ Sum Last 4 Quarters.
        # ä½†è¿™å¾ˆå¤æ‚.
        # æ­¤æ—¶ check 'net_income_ttm'. å¦‚æœæœ‰ net_income_ttm, æˆ‘ä»¬å¯ä»¥ç”¨ net_income_ttm / shares?
        # ä½† shares å†å²å¾ˆéš¾æ‰¾.
        # æ‰€ä»¥æœ€å¥½è¿˜æ˜¯ç”¨ 'eps' å­—æ®µ, å¹¶å‡è®¾å®ƒæ˜¯ TTM EPS (å¦‚æœ source æä¾›) æˆ–è€… Annual EPS.
        # å¯¹äºç¾è‚¡ (Yahoo), financial data varies.
        # è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ FinancialFundamentals schema.
        # ... 'revenue_ttm', 'net_income_ttm' exist.
        # So wait, we HAVE net_income_ttm!
        # Do we have Market Cap?
        # MarketDataDaily has 'market_cap'.
        # PE = Market Cap / Net Income TTM.
        # This is MUCH better if we have Market Cap history.
        # MarketDataDaily market_cap might be missing or derived?
        # If we have Close Price, we need Shares Outstanding History to get Market Cap History.
        # If we don't have Shares history, we can't get accurate Market Cap history.
        # So we stick to Price / EPS.
        # We need TTM EPS.
        # If financial_fundamentals has only Annual EPS, we use it (Static PE approx).
        # We will try to filter for Annual reports only? Or use provided data.
        
        financials = session.exec(
            select(FinancialFundamentals)
            .where(FinancialFundamentals.symbol == symbol)
            .order_by(FinancialFundamentals.as_of_date)
        ).all()
        
        if not financials:
            return None
            
        eps_list = []
        for f in financials:
            val = None
            # 1. ä¼˜å…ˆä½¿ç”¨ eps_ttm (å¦‚æœæœ‰)
            if hasattr(f, 'eps_ttm') and f.eps_ttm is not None:
                val = f.eps_ttm
            # 2. å…¶æ¬¡ä½¿ç”¨ eps (éœ€åˆ¤æ–­ Annual è¿˜æ˜¯ Quarterly)
            elif f.eps is not None:
                # ç®€å•å¤„ç†ï¼šå¦‚æœæ˜¯ Quarterly ä¸”æ²¡æœ‰ TTMï¼Œæš‚æ—¶å‡è®¾éœ€è¦ Rolling Sum (ä½†åœ¨ Loop é‡Œä¸å¥½åš Rolling)
                # æ›´å¥½çš„åšæ³•æ˜¯å…ˆè½¬ DataFrame å† Rolling Sum
                # è¿™é‡Œå…ˆåªæ”¶é›†åŸå§‹ EPS
                val = f.eps
            
            if val is not None:
                eps_list.append({
                    'date': f.as_of_date,
                    'eps': val,
                    'type': f.report_type,
                    'currency': f.currency if hasattr(f, 'currency') else None
                })
        
        if not eps_list:
            return None
            
        df_eps = pd.DataFrame(eps_list)
        df_eps['date'] = pd.to_datetime(df_eps['date'])
        df_eps = df_eps.set_index('date').sort_index()
        
        # --- Handle Quarterly to TTM ---
        # å¦‚æœå¤§éƒ¨åˆ†æ•°æ®æ˜¯ quarterlyï¼Œä¸”å€¼å¾ˆå°ï¼ˆç›¸æ¯”è‚¡ä»·ï¼‰ï¼Œè¯´æ˜æ˜¯å•å­£ EPS
        # éœ€è¦è®¡ç®— Rolling 4 Sum
        # ç®€å•é€»è¾‘ï¼šå¦‚æœæ˜¯ quarterlyï¼Œæ‰§è¡Œ rolling(4).sum()
        # æ³¨æ„ï¼šè¿™éœ€è¦æ•°æ®æ˜¯è¿ç»­çš„ quartersã€‚å¦‚æœä¸­é—´ç¼ºï¼Œsum ä¼šé”™ã€‚
        # ç¨³å¥åšæ³•ï¼š
        # 1. ç­›é€‰ quarterly rows
        # 2. resample('Q') å¡«å……ç¼ºå¤±? No, too complex.
        # 3. åªæ˜¯ç®€å• Rolling 4 sum, min_periods=4.
        
        # æ··åˆ Annual/Quarterly æ€ä¹ˆåŠï¼Ÿ
        # Strategy:
        # A. Separate Annual and Quarterly series.
        # B. If Quarterly exists and is sufficient, use Quarterly Rolling Sum. 
        # C. Fill gaps with Annual?
        # D. Simplify: If 'report_type' == 'quarterly', use rolling sum. If 'annual', use as is (static PE).
        
        # Check predominant type
        q_count = len(df_eps[df_eps['type'] == 'quarterly'])
        a_count = len(df_eps[df_eps['type'] == 'annual'])
        
        final_eps_series = None
        
        if q_count > a_count:
            # Main strategy: Quarterly Rolling Sum
            q_series = df_eps[df_eps['type'] == 'quarterly']['eps']
            # Rolling sum of last 4 (window=4)
            # Use 'time' based rolling? No, just periods if sorted.
            # Assuming ~4 reports per year.
            ttm_series = q_series.rolling(window=4, min_periods=1).sum() # min_periods=1 to have *some* data initially? No, TTM needs 4.
            # actually usually min_periods=4 for valid TTM. 
            # But let's be loose -> min_periods=1 allows partial TTM (better than nothing? No, dangerous. Partial sum is meaningless).
            # But if we use min_periods=4, early history is blank.
            # Let's use min_periods=4 but fillna with * 4 if only 1? No.
            # Let's try min_periods=4.
            ttm_series = q_series.rolling(window=4, min_periods=4).sum()
            final_eps_series = ttm_series
        else:
            # Annual strategy
            final_eps_series = df_eps['eps'] # Use as Static PE (Previous Year EPS)

        if final_eps_series is None or final_eps_series.empty:
             final_eps_series = df_eps['eps'] # Fallback to whatever we have

        # --- Currency Conversion ---
        # Check last record currency
        last_currency = df_eps.iloc[-1].get('currency')
        currency_multiplier = 1.0
        
        # Special Case for TSM (ADR vs TWD EPS)
        # TSM Price is USD (~190), EPS is TWD (~87). 
        # Need to divide EPS by TWD/USD rate (~32).
        # Or multiply Price by 32. 
        # PE = Price / EPS.
        # IF Price is USD, EPS is TWD. PE = USD / TWD = wrong.
        # We need PE = Price_USD / EPS_USD.
        # EPS_USD = EPS_TWD / ExchangeRate.
        # ExchangeRate ~ 32.5.
        
        if symbol == 'US:STOCK:TSM' and (last_currency == 'TWD' or last_currency is None):
             # Hardcode fix for TSM if currency not explicitly USD
             # Assuming EPS is TWD (87.2 is definitely TWD, USD EPS would be ~2.7)
             # EPS > 20 usually implies not USD for TSM.
             if final_eps_series.mean() > 20: 
                 currency_multiplier = 1.0 / 32.5 # approx rate
                 logger.info(f"  ğŸ’± TSM Detected: Converting TWD EPS to USD (Rate ~32.5)")

        # Generic Currency Handler
        # 1. US Market (Target: USD)
        if 'US:STOCK' in symbol and last_currency not in ['USD', None]:
            if last_currency in ['CNY', 'RMB']:
                 currency_multiplier = 1.0 / 7.2
            elif last_currency == 'TWD':
                 currency_multiplier = 1.0 / 32.5
            elif last_currency == 'HKD':
                 currency_multiplier = 1.0 / 7.8
            elif last_currency == 'JPY':
                 currency_multiplier = 1.0 / 150.0
        
        # 2. HK Market (Target: HKD)
        elif 'HK:STOCK' in symbol:
             # HK Stocks often report in CNY or USD
             if last_currency == 'USD':
                 currency_multiplier = 7.78  # USD -> HKD
             elif last_currency in ['CNY', 'RMB']:
                 currency_multiplier = 1.08  # CNY -> HKD (Approx, 1 CNY ~ 1.08 HKD)
                 # Wait, 1 CNY is STRONGER than HKD. 1 CNY = 1.08 HKD.
                 # EPS (CNY) * 1.08 = EPS (HKD). Correct.
             elif last_currency == 'HKD':
                 currency_multiplier = 1.0

        final_eps_series = final_eps_series * currency_multiplier

        # å°† EPS å¯¹é½åˆ°æ—¥çº¿ (FFill)
        eps_daily = final_eps_series.reindex(daily_prices.index, method='ffill')
        
        # è®¡ç®— PE
        pe_series = daily_prices['close'] / eps_daily
        pe_series = pe_series.replace([np.inf, -np.inf], np.nan).dropna()
        
        # Filter outliers? PE > 5000 or < 0
        pe_series = pe_series[(pe_series > 0) & (pe_series < 5000)]
        
        if pe_series.empty:
            return None
            
        return pd.DataFrame({'date': pe_series.index.strftime('%Y-%m-%d'), 'pe_ttm': pe_series.values})

    except Exception as e:
        logger.error(f"  âŒ æœ¬åœ°æ¨å¯¼å¤±è´¥ {symbol}: {e}")
        return None

# ==============================================================================
# Interactive CLI
# ==============================================================================

class Config:
    def __init__(self):
        self.markets = {'CN', 'HK', 'US'}
        # Default all selected
        self.selected_markets = self.markets.copy()

def clear_screen():
    print("\033[H\033[J", end="")

def print_menu(cfg: Config):
    clear_screen()
    print("="*60)
    print(" ğŸ“Š ä¼°å€¼æ•°æ®ä¸‹è½½å™¨ (Interactive) - ä»…æ”¯æŒä¸ªè‚¡ (STOCK)")
    print("="*60)
    
    def status(condition):
        return "âœ…" if condition else "âŒ"
    
    # Simple Market Toggles
    print(f" [1] {status('CN' in cfg.selected_markets)} CN")
    print(f" [2] {status('HK' in cfg.selected_markets)} HK")
    print(f" [3] {status('US' in cfg.selected_markets)} US")
    
    print("-" * 60)
    print(" [0] â–¶ï¸  å¼€å§‹æ›´æ–°     [A] å…¨é€‰     [C] æ¸…ç©º")
    print(" [Q] é€€å‡º")
    print("="*60)

def configure():
    cfg = Config()
    
    # Mapping keys to toggle actions
    toggles = {
        '1': lambda: toggle(cfg.selected_markets, 'CN'),
        '2': lambda: toggle(cfg.selected_markets, 'HK'),
        '3': lambda: toggle(cfg.selected_markets, 'US'),
    }
    
    while True:
        print_menu(cfg)
        try:
            choice = input(" è¯·è¾“å…¥é€‰é¡¹ [0-9/A/C]: ").strip().upper()
        except (EOFError, KeyboardInterrupt):
            print("\né€€å‡º")
            sys.exit(0)
            
        if choice == '0':
            return cfg
        elif choice == 'Q':
            sys.exit(0)
        elif choice in toggles:
            toggles[choice]()
        elif choice == 'A':
            cfg.selected_markets = cfg.markets.copy()
        elif choice == 'C':
            cfg.selected_markets.clear()
            
def toggle(selection_set, item):
    if item in selection_set:
        selection_set.remove(item)
    else:
        selection_set.add(item)

# ==============================================================================
# Futu Interface
# ==============================================================================
def fetch_hk_valuation_futu(symbol: str, market: str, session: Session):
    """
    Use Futu OpenD to fetch historical PE for HK stocks.
    Requires FutuOpenD running on 127.0.0.1:11111.
    """
    try:
        from futu import OpenQuoteContext, KLType, AuType, RET_OK
        import datetime
        
        # Futu Code Format: HK:STOCK:00700 -> HK.00700
        futu_code = symbol.replace('HK:STOCK:', 'HK.')
        
        # Connect
        quote_ctx = OpenQuoteContext(host='127.0.0.1', port=11111)
        
        # Fetch History with Pagination
        end_str = datetime.datetime.now().strftime("%Y-%m-%d")
        start_str = "2020-01-01"
        
        all_data = []
        page_req_key = None
        
        while True:
            ret, data, page_req_key = quote_ctx.request_history_kline(
                code=futu_code,
                start=start_str,
                end=end_str,
                ktype=KLType.K_DAY,
                autype=AuType.QFQ,
                max_count=1000,
                page_req_key=page_req_key
            )
            
            if ret == RET_OK:
                if not data.empty:
                    all_data.append(data)
            else:
                logger.error(f"  âŒ Futu Error {symbol}: {data}")
                break
                
            if page_req_key is None:
                break
                
        if not all_data:
            logger.warning(f"  âš ï¸  Futu Empty {symbol}")
            quote_ctx.close()
            return
            
        import pandas as pd
        data = pd.concat(all_data, ignore_index=True)

            
        # Parse and Save
        # Data columns: time_key, pe_ratio, ...
        # Futu PE is Static.
        
        # Check for Time Shift (Simulation Mode support)
        # If System Time is 2026 but Futu returns 2025, we shift Futu dates to match DB.
        system_year = datetime.datetime.now().year
        futu_latest_str = data.iloc[-1]['time_key']
        futu_latest_year = int(futu_latest_str.split('-')[0])
        
        year_offset = 0
        if system_year > futu_latest_year:
             year_offset = system_year - futu_latest_year
             logger.info(f"  ğŸ•°ï¸  Detected Simulation Mode: Shifting Futu data by +{year_offset} years")
        
        updates = []
        for _, row in data.iterrows():
            futu_date_str = row['time_key'].split(' ')[0] # 2025-01-01
            pe_val = row['pe_ratio']
            
            # Apply Shift
            if year_offset > 0:
                 futu_dt = datetime.datetime.strptime(futu_date_str, "%Y-%m-%d")
                 shifted_dt = futu_dt.replace(year=futu_dt.year + year_offset)
                 date_str = shifted_dt.strftime("%Y-%m-%d")
            else:
                 date_str = futu_date_str
            
            # Map Close Time (HK 16:00)
            timestamp_str = f"{date_str} 16:00:00"
            
            # Find DB Record
            stmt = select(MarketDataDaily).where(
                MarketDataDaily.symbol == symbol,
                MarketDataDaily.market == market,
                MarketDataDaily.timestamp == timestamp_str
            )
            record = session.exec(stmt).first()
            
            if record:
                if pe_val and pe_val > 0:
                    record.pe = float(pe_val) # Save to Static PE
                    # record.pe_ttm = None    # Keep TTM clean or update from Snapshot later
                    record.updated_at = datetime.datetime.now()
                    session.add(record)
                    updates.append(1)
        
        # --- NEW: Fetch Snapshot for Latest TTM ---
        # Only if we have a record for Today (or latest available)
        try:
            ret_s, data_s = quote_ctx.get_market_snapshot([futu_code])
            if ret_s == RET_OK and not data_s.empty:
                pe_ttm = data_s.iloc[0].get('pe_ttm_ratio')
                pe_static = data_s.iloc[0].get('pe_ratio')
                
                if pe_ttm and pe_ttm > 0:
                     # Find TODAY's record (System Date)
                     today_str = datetime.datetime.now().strftime("%Y-%m-%d")
                     ts_today = f"{today_str} 16:00:00"
                     
                     rec_today = session.exec(select(MarketDataDaily).where(
                         MarketDataDaily.symbol == symbol,
                         MarketDataDaily.timestamp == ts_today
                     )).first()
                     
                     if rec_today:
                         rec_today.pe_ttm = float(pe_ttm)
                         # Optionally update PE Static if Snapshot is fresher? 
                         # But History loop usually covers it.
                         # rec_today.pe = float(pe_static) 
                         session.add(rec_today)
                         logger.info(f"  ğŸ“¸ Snapshot TTM Updated: {symbol} -> {pe_ttm}")
                         updates.append(1)
            else:
                logger.warning(f"  âš ï¸  Snapshot Failed {symbol}: {data_s}")
        except Exception as e:
            logger.error(f"  âŒ Snapshot Error {symbol}: {e}")

        session.commit()
        quote_ctx.close()

        if updates:
             logger.info(f"  âœ… Futu Saved {symbol}: {len(updates)} records (Static PE)")
             
    except ImportError:
        logger.error("  âŒ Futu API not installed (pip install futu-api)")
    except Exception as e:
         logger.error(f"  âŒ Futu Exception {symbol}: {e}")


# ==============================================================================
# Main
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description='è·å–å†å²ä¼°å€¼æ•°æ® (PE/PB)')
    parser.add_argument('--symbol', type=str, help='æŒ‡å®šè¦å¤„ç†çš„ Canonical ID (ä¾‹å¦‚ US:STOCK:TSLA, HK:STOCK:00700)')
    args = parser.parse_args()

    # 1. Configuration (Interactive vs Headless)
    selected_markets = {'CN', 'HK', 'US'}
    
    if args.symbol:
        print(f"ğŸ¯ ä»…å¤„ç†æŒ‡å®šèµ„äº§: {args.symbol}")
        # Infer market from symbol if possible, or just rely on symbol filter later
        # But we need selected_markets for the query logic below
        parts = args.symbol.split(':')
        if len(parts) > 0 and parts[0] in selected_markets:
            selected_markets = {parts[0]}
    else:
        # Interactive Mode
        print("è¿›å…¥äº¤äº’æ¨¡å¼...")
        try:
            cfg = configure()
            selected_markets = cfg.selected_markets
        except KeyboardInterrupt:
            print("\nExit")
            return

    print("=" * 80)
    print(f"ğŸ“Š å¼€å§‹è·å–ä¼°å€¼æ•°æ® (Markets: {selected_markets})")
    print("   æ³¨æ„: ä»…æ›´æ–°ä¸ªè‚¡ (STOCK)")
    print("=" * 80)
    
    try:
        with Session(engine) as session:
            # æ„å»ºæŸ¥è¯¢
            stmt = select(Watchlist).where(Watchlist.market.in_(selected_markets))
            
            # å¦‚æœæŒ‡å®šäº† symbol, å¢åŠ è¿‡æ»¤æ¡ä»¶
            if args.symbol:
                stmt = stmt.where(Watchlist.symbol == args.symbol)
                
            watchlist = session.exec(stmt).all()
            
            if not watchlist:
                print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„èµ„äº§" + (f": {args.symbol}" if args.symbol else ""))
                return
            
            # é¢„å…ˆè¿‡æ»¤: åªä¿ç•™ STOCK
            targets = []
            for item in watchlist:
                parts = item.symbol.split(':')
                # Canonical ID Format: MARKET:TYPE:CODE
                # But sometimes simple formats exist. Let's use robust check.
                # Assuming format is strictly followed or at least contains type.
                if len(parts) >= 2:
                    asset_type = parts[1]
                else:
                    # Fallback or skip? safely skip if unsure
                    continue
                    
                if asset_type == 'STOCK':
                    targets.append(item)
            
            count_total = len(targets)
            if count_total == 0:
                print("âš ï¸  ç­›é€‰åæ— ä¸ªè‚¡ç›®æ ‡ (STOCK).")
                return

            print(f"\nå…± {count_total} ä¸ªä¸ªè‚¡èµ„äº§éœ€è¦è·å–ä¼°å€¼æ•°æ®\n")
            
            cn_count = 0
            hk_count = 0
            us_count = 0

            for idx, item in enumerate(targets, 1):
                print(f"\n[{idx}/{count_total}] {'='*60}")
                
                # Check for strict interruption
                
                parts = item.symbol.split(':')
                asset_type = parts[1] if len(parts) >= 2 else 'STOCK'
                
                try:
                    if item.market == 'CN':
                        # Aè‚¡ä¿æŒåŸæœ‰é€»è¾‘
                        df = fetch_cn_valuation_history(item.symbol, asset_type)
                        
                        if df is not None and not df.empty:
                            dividend_yield = fetch_cn_dividend_yield(item.symbol)
                            if dividend_yield is not None:
                                df['dividend_yield'] = dividend_yield
                            
                            updated = save_cn_valuation_to_daily(item.symbol, df, session)
                            if updated > 0:
                                cn_count += 1
                                
                    elif item.market == 'HK':
                        print(f"ğŸ”„ å¤„ç†æ¸¯è‚¡ (Futu + Dividend): {item.symbol}")
                        # 1. Futu: PE/PE_TTM/PB
                        fetch_hk_valuation_futu(item.symbol, "HK", session)
                        
                        # 2. Yahoo: Dividend Yield (Restore)
                        div_yield = fetch_hk_dividend_yield(item.symbol)
                        if div_yield is not None:
                             # Save Dividend Yield to Database (Find latest record)
                             # Reuse save_cn_valuation_to_daily logic or inline simple update?
                             # Let's verify if save_hk_valuation_to_daily supports dividend? No.
                             # Simple inline update for today/latest
                             try:
                                 latest_rec = session.exec(
                                     select(MarketDataDaily).where(
                                         MarketDataDaily.symbol == item.symbol, 
                                         MarketDataDaily.market == 'HK'
                                     ).order_by(MarketDataDaily.timestamp.desc())
                                 ).first()
                                 if latest_rec:
                                     latest_rec.dividend_yield = div_yield
                                     latest_rec.updated_at = datetime.now()
                                     session.add(latest_rec)
                                     session.commit()
                                     print(f"  ğŸ’¾ å·²ä¿å­˜è‚¡æ¯ç‡: {div_yield}%")
                             except Exception as e:
                                 logger.error(f"  âŒ ä¿å­˜è‚¡æ¯ç‡å¤±è´¥: {e}")

                        hk_count += 1


                    elif item.market == 'US':
                        print(f"ğŸ”„ å¤„ç†ç¾è‚¡: {item.symbol}")
                        # 1. å®æ—¶ä¼°å€¼ (yfinance)
                        valuation = fetch_us_valuation_yfinance(item.symbol)
                        if valuation:
                            updated = save_us_valuation_to_daily(item.symbol, valuation, session)
                            if updated > 0:
                                us_count += 1 
                        
                        # 2. å†å²ä¼°å€¼ (FMP)
                        df_fmp = fetch_us_valuation_history_fmp(item.symbol, limit=20) 
                        
                        if df_fmp is not None and not df_fmp.empty:
                            save_us_historical_valuation_to_daily(item.symbol, df_fmp, session)
                            
                            # 3. æ¨å¯¼ (Base on FMP)
                            try:
                                daily_prices = pd.read_sql(
                                    select(MarketDataDaily.timestamp, MarketDataDaily.close)
                                    .where(MarketDataDaily.symbol == item.symbol)
                                    .order_by(MarketDataDaily.timestamp),
                                    engine
                                )
                                if not daily_prices.empty:
                                    daily_prices['timestamp'] = pd.to_datetime(daily_prices['timestamp'])
                                    daily_prices.set_index('timestamp', inplace=True)
                                    
                                    derived_pe = derive_daily_pe_from_points(item.symbol, daily_prices, df_fmp)
                                    if not derived_pe.empty:
                                        logger.info(f"  ğŸ“ˆ [Derivation] US: æ¨å¯¼å¹¶ä¿å­˜ {len(derived_pe)} æ¡ PE TTM (Based on FMP)")
                                        df_derived = pd.DataFrame({'date': derived_pe.index.strftime('%Y-%m-%d'), 'pe_ttm': derived_pe.values})
                                        save_us_historical_valuation_to_daily(item.symbol, df_derived, session)
                            except Exception as e:
                                logger.error(f"  âŒ ç¾è‚¡ FMP æ¨å¯¼é€»è¾‘å¼‚å¸¸: {e}")
                                
                        # 4. [New Fallback] å¦‚æœ FMP å¤±è´¥ (df_fmp is None), å°è¯•æœ¬åœ°è´¢æŠ¥æ¨å¯¼
                        if df_fmp is None or df_fmp.empty:
                            logger.info(f"  âš ï¸  FMP å¤±è´¥æˆ–æ— æ•°æ®ï¼Œå°è¯•æœ¬åœ°è´¢æŠ¥æ¨å¯¼...")
                            derived_local = derive_pe_ttm_from_fundamentals(item.symbol, session)
                            if derived_local is not None and not derived_local.empty:
                                logger.info(f"  ğŸ“ˆ [Fallback] US: ä½¿ç”¨æœ¬åœ°è´¢æŠ¥æ¨å¯¼ {len(derived_local)} æ¡ PE TTM")
                                save_us_historical_valuation_to_daily(item.symbol, derived_local, session)

                    # é¿å…è¯·æ±‚è¿‡å¿«

                    # é¿å…è¯·æ±‚è¿‡å¿«
                    import time
                    time.sleep(0.5)

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¤±è´¥ {item.symbol}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

            # æ€»ç»“
            print("\n" + "=" * 80)
            print("ğŸ“‹ è·å–å®Œæˆç»Ÿè®¡")
            print("=" * 80)
            print(f"âœ… Aè‚¡æˆåŠŸ: {cn_count} ä¸ª")
            print(f"âœ… æ¸¯è‚¡æˆåŠŸ: {hk_count} ä¸ª")
            print(f"âœ… ç¾è‚¡æˆåŠŸ: {us_count} ä¸ª")
            print("=" * 80)
            
    except Exception as e:
         logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
         import traceback
         traceback.print_exc()

if __name__ == "__main__":
    main()
