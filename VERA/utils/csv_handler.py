import pandas as pd
import sqlite3
from db.connection import get_connection
from utils.canonical_resolver import resolve_canonical_symbol

def parse_and_import_csv(uploaded_file, fallback_id=None, fallback_name=None, mode="overwrite", start_date=None, end_date=None, target_assets=None):
    """
    è§£æç”¨æˆ·ä¸Šä¼ çš„ CSV å¹¶å­˜å…¥ vera_price_cache
    æ ¸å¿ƒè§„åˆ™ï¼š
    1. ä»…ä½¿ç”¨å·²æœ‰çš„ç¬¦å·æ˜ å°„å…³ç³»ï¼ˆä¸åˆ›å»ºæ–°æ˜ å°„ï¼‰
    2. æœªæ˜ å°„çš„ä»£ç ä¼šè¢«è·³è¿‡å¹¶æŠ¥å‘Šç»™ç”¨æˆ·
    3. è¿”å›è¯¦ç»†çš„å¯¼å…¥æ‘˜è¦
    
    Args:
        mode: "overwrite" (æ›´æ–°å·²æœ‰è®°å½•) or "incremental" (ä»…æ’å…¥æ–°è®°å½•)
    """
    try:
        # 1. è¯»å– CSV
        df = pd.read_csv(uploaded_file)
        
        # 2. åˆ—åæ¸…æ´—ï¼ˆå¯å‘å¼åŒ¹é…ï¼‰
        raw_columns = list(df.columns)
        df.columns = [c.strip().lower() for c in df.columns]
        
        # æ˜ å°„é€»è¾‘
        mapping = {
            'date': ['date', 'time', 'timestamp', 'æ—¥æœŸ'],
            'close': ['close', 'adj close', 'æ”¶ç›˜ä»·', 'æˆäº¤ä»·'],
            'open': ['open', 'å¼€ç›˜ä»·'],
            'high': ['high', 'æœ€é«˜ä»·'],
            'low': ['low', 'æœ€ä½ä»·'],
            'volume': ['volume', 'æˆäº¤é‡'],
            'symbol': ['symbol', 'ticker', 'code', 'ä»£ç ', 'æ ‡çš„']
        }
        
        # åŒ¹é…å¿…éœ€åˆ—
        date_col = next((c for c in df.columns if any(k == c for k in mapping['date'])), None)
        close_col = next((c for c in df.columns if any(k == c for k in mapping['close'])), None)
        symbol_col = next((c for c in df.columns if any(k == c for k in mapping['symbol'])), None)
        
        if not date_col or not close_col:
            return False, f"CSV ç¼ºå°‘å¿…éœ€åˆ— (éœ€åŒ…å«æ—¥æœŸå’Œæ”¶ç›˜ä»·)ã€‚å·²å‘ç°åˆ—: {raw_columns}"
            
        # å‡†å¤‡æ¸…æ´—åçš„åŸºç¡€æ•°æ®
        cleaned_data = pd.DataFrame()
        cleaned_data['trade_date'] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')
        cleaned_data['close'] = pd.to_numeric(df[close_col], errors='coerce')
        
        # å¤„ç† Symbol
        if symbol_col:
            cleaned_data['raw_symbol'] = df[symbol_col].astype(str).str.strip().str.upper()
        else:
            if not fallback_id:
                return False, "CSV ä¸­æœªå‘ç°ä»£ç åˆ—ï¼Œä¸”æœªæä¾›å¤‡ç”¨ä»£ç ã€‚"
            cleaned_data['raw_symbol'] = fallback_id.strip().upper()

        # åŒ¹é…å¯é€‰åˆ—
        for col in ['open', 'high', 'low']:
            match = next((c for c in df.columns if any(k == c for k in mapping[col])), None)
            cleaned_data[col] = pd.to_numeric(df[match], errors='coerce') if match else cleaned_data['close']
            
        vol_match = next((c for c in df.columns if any(k == c for k in mapping['volume'])), None)
        cleaned_data['volume'] = pd.to_numeric(df[vol_match], errors='coerce') if vol_match else 0

        # æ–°å¢æŒ‡æ ‡æ˜ å°„æ”¯æŒ (v2 åŒæ­¥æ‰©å±•)
        extended_metrics = {
            'pe': ['pe', 'pe_static', 'å¸‚ç›ˆç‡', 'é™æ€å¸‚ç›ˆç‡'],
            'pe_ttm': ['pe_ttm', 'ttm_pe', 'åŠ¨æ€PE', 'åŠ¨æ€å¸‚ç›ˆç‡', 'å¸‚ç›ˆç‡TTM'],
            'pb': ['pb', 'pb_ratio', 'å¸‚å‡€ç‡'],
            'ps': ['ps', 'ps_ttm', 'å¸‚é”€ç‡'],
            'eps': ['eps', 'eps_ttm', 'æ¯è‚¡æ”¶ç›Š'],
            'dividend_yield': ['dividend_yield', 'è‚¡æ¯ç‡'],
            'turnover': ['turnover', 'æ¢æ‰‹ç‡'],
            'market_cap': ['market_cap', 'å¸‚å€¼', 'æ€»å¸‚å€¼'],
            'pct_change': ['pct_change', 'ç™¾åˆ†æ¯”æ¶¨è·Œ', 'æ¶¨è·Œå¹…'],
            'prev_close': ['prev_close', 'å‰æ”¶ç›˜ä»·']
        }
        for col, keys in extended_metrics.items():
            match = next((c for c in df.columns if any(k == c for k in keys)), None)
            if match:
                cleaned_data[col] = pd.to_numeric(df[match], errors='coerce')
            else:
                cleaned_data[col] = None
        
        # æ¸…æ´—æ— æ•ˆè¡Œ
        cleaned_data = cleaned_data.dropna(subset=['trade_date', 'close', 'raw_symbol'])
        
        # Date Range Filtering
        if start_date:
            cleaned_data = cleaned_data[cleaned_data['trade_date'] >= str(start_date)]
        if end_date:
            cleaned_data = cleaned_data[cleaned_data['trade_date'] <= str(end_date)]

        
        if cleaned_data.empty:
            return False, "æ¸…æ´—åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ CSV å†…å®¹æ ¼å¼ã€‚"

        # 3. è§£ææ‰€æœ‰symbolåˆ°canonical IDï¼ˆé€šè¿‡å·²æœ‰æ˜ å°„ï¼‰
        conn = get_connection()
        try:
            symbols_in_csv = cleaned_data['raw_symbol'].unique()
            symbol_resolution = {}  # {raw_symbol: canonical_id or None}
            
            for raw_sym in symbols_in_csv:
                try:
                    canonical = resolve_canonical_symbol(conn, raw_sym)
                    symbol_resolution[raw_sym] = canonical
                except:
                    symbol_resolution[raw_sym] = None  # æœªæ‰¾åˆ°æ˜ å°„
            
            # Asset Filter Logic
            if target_assets:
                # Only keep mappings where the canonical ID is in the target list
                symbol_resolution = {k: v for k, v in symbol_resolution.items() if v in target_assets}

            
            # ç»Ÿè®¡
            mapped_symbols = {k: v for k, v in symbol_resolution.items() if v}
            unmapped_symbols = {k for k, v in symbol_resolution.items() if not v}
            
            # è¿‡æ»¤ï¼šåªä¿ç•™å·²æ˜ å°„çš„æ•°æ®
            cleaned_data['canonical_id'] = cleaned_data['raw_symbol'].map(symbol_resolution)
            valid_data = cleaned_data[cleaned_data['canonical_id'].notna()].copy()
            skipped_data = cleaned_data[cleaned_data['canonical_id'].isna()]
            
            if valid_data.empty:
                return False, f"âŒ æ‰€æœ‰ä»£ç å‡æœªåœ¨ç³»ç»Ÿä¸­æ³¨å†Œï¼Œæ— æ³•å¯¼å…¥ã€‚\n\næœªæ³¨å†Œä»£ç : {', '.join(unmapped_symbols)}\n\nè¯·å…ˆåœ¨ã€Œèµ„äº§ç®¡ç†ã€ä¸­æ³¨å†Œè¿™äº›èµ„äº§ã€‚"
            
            # å‡†å¤‡å†™å…¥æ•°æ®
            valid_data['symbol'] = valid_data['canonical_id']  # price_cacheä½¿ç”¨canonicalä½œä¸ºsymbol
            valid_data['source'] = 'User_Upload_CSV'
            
            # åŠ¨æ€é€‰æ‹©åˆ—ï¼Œç¡®ä¿æ–°å­—æ®µè¢«åŒ…å«
            cols_to_save = ['symbol', 'trade_date', 'open', 'high', 'low', 'close', 'volume', 'source',
                            'pe', 'pe_ttm', 'pb', 'ps', 'eps', 'dividend_yield', 'turnover', 'market_cap', 
                            'pct_change', 'prev_close']
            # åªå– valid_data ä¸­å­˜åœ¨çš„åˆ—
            actual_cols = [c for c in cols_to_save if c in valid_data.columns]
            valid_data = valid_data[actual_cols]
            
            # 4. å†™å…¥æ•°æ®åº“ï¼ˆåˆ†æ‰¹é¿å… SQL å˜é‡è¶…é™ï¼‰
            BATCH_SIZE = 100  # æ¯æ‰¹100è¡Œï¼Œé¿å…è¶…è¿‡SQLite 999å˜é‡é™åˆ¶
            
            # ç»Ÿè®¡æ¯ä¸ªèµ„äº§çš„æ–°å¢å’Œé‡å¤
            asset_stats = {}  # {canonical_id: {'total': X, 'inserted': Y, 'duplicate': Z}}
            
            for canonical_id, group in valid_data.groupby('symbol'):
                total_rows = len(group)
                inserted = 0
                duplicated = 0
                
                # åˆ†æ‰¹æ’å…¥
                for i in range(0, len(group), BATCH_SIZE):
                    batch = group.iloc[i:i+BATCH_SIZE]
                    
                    # ä½¿ç”¨ INSERT OR IGNORE å¹¶æ£€æŸ¥å®é™…æ’å…¥è¡Œæ•° (åŠ¨æ€æ„å»º SQL ä»¥é€‚é…æ–°å­—æ®µ)
                    cursor = conn.cursor()
                    col_names = ", ".join(actual_cols)
                    placeholders = ", ".join(["?"] * len(actual_cols))
                    
                    # Mode-dependent SQL logic
                    if mode == "incremental":
                        # Incremental: Skip existing records
                        sql = f"""
                            INSERT OR IGNORE INTO vera_price_cache ({col_names})
                            VALUES ({placeholders})
                        """
                    else:
                        # Overwrite: Update existing records
                        update_clause = ", ".join([f"{c} = excluded.{c}" for c in actual_cols if c not in ['symbol', 'trade_date']])
                        sql = f"""
                            INSERT INTO vera_price_cache ({col_names})
                            VALUES ({placeholders})
                            ON CONFLICT(symbol, trade_date) DO UPDATE SET
                            {update_clause}
                        """
                    
                    cursor.executemany(sql, [tuple(row) for _, row in batch.iterrows()])
                    
                    actual_inserted = cursor.rowcount
                    inserted += actual_inserted
                    duplicated += (len(batch) - actual_inserted)
                
                asset_stats[canonical_id] = {
                    'total': total_rows,
                    'inserted': inserted,
                    'duplicate': duplicated
                }
            
            conn.commit()
            
            # 5. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            total_inserted = sum(s['inserted'] for s in asset_stats.values())
            total_duplicate = sum(s['duplicate'] for s in asset_stats.values())
            
            mode_label = "å…¨è¦†ç›–" if mode == "overwrite" else "å¢é‡æ·»åŠ "
            report_lines = [f"âœ… CSV å¯¼å…¥å®Œæˆ (æ¨¡å¼: {mode_label})\n"]
            
            # å…ˆæ˜¾ç¤ºæ±‡æ€»
            report_lines.append(f"**ğŸ“Š æ±‡æ€»**")
            report_lines.append(f"  - æˆåŠŸå¯¼å…¥èµ„äº§: **{len(mapped_symbols)}** ä¸ª")
            report_lines.append(f"  - æ–°å¢è®°å½•: **{total_inserted}** æ¡")
            
            if mode == "incremental":
                report_lines.append(f"  - è·³è¿‡å·²å­˜åœ¨: **{total_duplicate}** æ¡")
            else:
                report_lines.append(f"  - æ›´æ–°å·²æœ‰: **{total_duplicate}** æ¡")
            report_lines.append("")  # ç©ºè¡Œåˆ†éš”
            
            # å†æ˜¾ç¤ºåˆ†é¡¹
            if len(mapped_symbols) > 0:
                report_lines.append(f"**ğŸ“‹ åˆ†é¡¹è¯¦æƒ…**\n")
                for raw, canonical in mapped_symbols.items():
                    stats = asset_stats.get(canonical, {'total': 0, 'inserted': 0, 'duplicate': 0})
                    report_lines.append(f"**{raw}** â†’ `{canonical}`")
                    report_lines.append(f"  - CSVæ€»è¡Œæ•°: {stats['total']}")
                    report_lines.append(f"  - æ–°å¢: {stats['inserted']} æ¡")
                    if stats['duplicate'] > 0:
                        report_lines.append(f"  - é‡å¤: {stats['duplicate']} æ¡")
                    report_lines.append("")  # ç©ºè¡Œåˆ†éš”
            
            if unmapped_symbols:
                report_lines.append(f"âš ï¸ **æœªèƒ½å¯¼å…¥ä»£ç ** ({len(unmapped_symbols)} ä¸ª):")
                report_lines.append(f"  {', '.join(sorted(unmapped_symbols))}")
                report_lines.append(f"\nğŸ’¡ **å»ºè®®**: è¯·å…ˆåœ¨ã€Œèµ„äº§ç®¡ç†ã€é¡µé¢æ³¨å†Œè¿™äº›èµ„äº§ï¼Œç„¶åé‡æ–°å¯¼å…¥ CSVã€‚")
            
            # # è¡¥å……è´¢åŠ¡æ•°æ®ï¼ˆå·²æ³¨é‡Šï¼šå½“å‰CSVæ–‡ä»¶å·²åŒ…å«PE/PB/EPSæ•°æ®ï¼‰
            # if len(mapped_symbols) > 0:
            #     report_lines.append(f"\nğŸ“Š **æ­£åœ¨è¡¥å……è´¢åŠ¡æ•°æ®...**")
            #     try:
            #         from utils.financial_supplement import batch_supplement_financials
            #         
            #         unique_canonical_ids = list(set(mapped_symbols.values()))
            #         fin_stats = batch_supplement_financials(unique_canonical_ids, verbose=False)
            #         
            #         report_lines.append(f"  - æˆåŠŸ: {fin_stats['success']} ä¸ª")
            #         report_lines.append(f"  - å¤±è´¥: {fin_stats['failed']} ä¸ª")
            #         report_lines.append(f"  - è·³è¿‡ï¼ˆå·²æœ‰æ•°æ®ï¼‰: {fin_stats['skipped']} ä¸ª")
            #         
            #         if fin_stats['failed'] > 0:
            #             report_lines.append(f"\nğŸ’¡ **æç¤º**: éƒ¨åˆ†èµ„äº§å¯èƒ½åœ¨ Yahoo Finance ä¸­æ— è´¢åŠ¡æ•°æ®")
            #     except Exception as e:
            #         report_lines.append(f"  âš ï¸ è´¢åŠ¡æ•°æ®è¡¥å……å¤±è´¥: {str(e)}")
            
            return True, "\n".join(report_lines)
            
        except Exception as e:
            return False, f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"
        finally:
            conn.close()
            
    except Exception as e:
        return False, f"CSV è§£æå¤±è´¥: {str(e)}"



def parse_and_import_financials_csv(uploaded_file, unit_scale=100_000_000, mode="overwrite", start_date=None, end_date=None, target_assets=None):
    """
    Parse and import financial historical data from CSV.
    Populates financial_history and financial_fundamentals.
    
    Args:
        mode: "overwrite" (æ›´æ–°å·²æœ‰è®°å½•) or "incremental" (ä»…æ’å…¥æ–°è®°å½•)
        target_assets (list): Optional. List of canonical IDs to strictly filter import data.
    """
    try:
        df = pd.read_csv(uploaded_file)
        df.columns = [c.strip().lower() for c in df.columns]
        
        conn = get_connection()
        try:
            # Column mapping
            mapping = {
                'symbol': ['symbol', 'ticker', 'code', 'ä»£ç ', 'æ ‡çš„'],
                'date': ['as_of_date', 'date', 'report_date', 'æ—¥æœŸ', 'æˆªæ­¢æ—¥æœŸ'],
                'revenue': ['revenue', 'revenue_ttm', 'è¥æ”¶', 'è¥ä¸šæ”¶å…¥'],
                'net_income': ['net_income', 'net_profit', 'net_income_ttm', 'net_profit_ttm', 'å‡€åˆ©æ¶¦'],
                'currency': ['currency', 'è´§å¸', 'å¸ç§']
            }
            
            # Simple column resolver
            def find_col(keys):
                for c in df.columns:
                    if any(k in c for k in keys): return c
                return None
            
            sym_col = find_col(mapping['symbol'])
            date_col = find_col(mapping['date'])
            rev_col = find_col(mapping['revenue'])
            ni_col = find_col(mapping['net_income'])
            cur_col = find_col(mapping['currency'])
            
            if not sym_col or not date_col:
                return False, f"CSV ç¼ºå°‘å¿…éœ€åˆ— (éœ€åŒ…å«ä»£ç å’Œæ—¥æœŸ)ã€‚å·²å‘ç°åˆ—: {list(df.columns)}"
            
            # æ›´å…¨é¢çš„æ–°å­—æ®µæ˜ å°„
            ext_mapping = {
                'op_cf': ['operating_cashflow_ttm', 'ç»è¥ç°é‡‘æµ', 'ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢'],
                'free_cf': ['free_cashflow_ttm', 'è‡ªç”±ç°é‡‘æµ'],
                'total_assets': ['total_assets', 'æ€»èµ„äº§', 'èµ„äº§æ€»è®¡'],
                'total_liabs': ['total_liabilities', 'æ€»è´Ÿå€º', 'è´Ÿå€ºåˆè®¡'],
                'total_debt': ['total_debt', 'æ€»å€ºåŠ¡', 'æœ‰æ¯è´Ÿå€º'],
                'cash': ['cash_and_equivalents', 'ç°é‡‘', 'è´§å¸èµ„é‡‘'],
                'net_debt': ['net_debt', 'å‡€å€ºåŠ¡'],
                'de_ratio': ['debt_to_equity', 'äº§æƒæ¯”ç‡', 'è´Ÿå€ºæƒç›Šæ¯”'],
                'ic_ratio': ['interest_coverage', 'åˆ©æ¯ä¿éšœå€æ•°'],
                'curr_ratio': ['current_ratio', 'æµåŠ¨æ¯”ç‡'],
                'div_yield': ['dividend_yield', 'è‚¡æ¯ç‡'],
                'payout': ['payout_ratio', 'åˆ†çº¢ç‡', 'è‚¡åˆ©æ”¯ä»˜ç‡'],
                'buyback': ['buyback_ratio', 'å›è´­ç‡']
            }
            
            def find_ext_val(row, keys):
                match = next((c for c in df.columns if any(k == c for k in keys)), None)
                return row.get(match) if match else None

            def scale_it(val):
                if pd.isna(val) or val == '': return None
                try:
                    return float(val) * unit_scale
                except:
                    return None
            
            def get_raw(val):
                if pd.isna(val) or val == '': return None
                return val

            inserted = 0
            updated = 0
            errors = 0
            
            for _, row in df.iterrows():
                try:
                    raw_symbol = str(row[sym_col]).strip()
                    as_of_date = str(row[date_col]).strip()
                    if not as_of_date or as_of_date == 'nan': continue
                    
                    # Convert to YYYY-MM-DD for comparison and storage consistency
                    try:
                        report_date_obj = pd.to_datetime(as_of_date)
                        as_of_date_str = report_date_obj.strftime('%Y-%m-%d')
                    except:
                        # Continue if date parse fails, let original logic handle or skip
                        continue

                    # Date Range Filtering
                    if start_date and as_of_date_str < str(start_date): continue
                    if end_date and as_of_date_str > str(end_date): continue
                    
                    # Use standarized date string
                    as_of_date = as_of_date_str

                    
                    # Resolve Canonical
                    asset_id = resolve_canonical_symbol(conn, raw_symbol)
                    if not asset_id: asset_id = raw_symbol
                    
                    # Asset Filter Logic
                    if target_assets and asset_id not in target_assets:
                        continue
                    
                    revenue = scale_it(row.get(rev_col)) if rev_col else None
                    net_profit = scale_it(row.get(ni_col)) if ni_col else None
                    currency = str(row.get(cur_col)).strip() if cur_col and not pd.isna(row.get(cur_col)) else 'CNY'
                    
                    # Optional Fundamental metrics (Using scale_it/get_raw with find_ext_val)
                    op_cf = scale_it(find_ext_val(row, ext_mapping['op_cf']))
                    free_cf = scale_it(find_ext_val(row, ext_mapping['free_cf']))
                    t_assets = scale_it(find_ext_val(row, ext_mapping['total_assets']))
                    t_liab = scale_it(find_ext_val(row, ext_mapping['total_liabs']))
                    t_debt = scale_it(find_ext_val(row, ext_mapping['total_debt']))
                    cash = scale_it(find_ext_val(row, ext_mapping['cash']))
                    n_debt = scale_it(find_ext_val(row, ext_mapping['net_debt']))
                    d_e = get_raw(find_ext_val(row, ext_mapping['de_ratio']))
                    int_cov = get_raw(find_ext_val(row, ext_mapping['ic_ratio']))
                    curr_ratio = get_raw(find_ext_val(row, ext_mapping['curr_ratio']))
                    div_yield = get_raw(find_ext_val(row, ext_mapping['div_yield']))
                    payout = get_raw(find_ext_val(row, ext_mapping['payout']))
                    buyback = get_raw(find_ext_val(row, ext_mapping['buyback']))
                    
                    cursor = conn.cursor()
                    
                    # 1. Update/Insert financial_history
                    # åˆ—æ˜ å°„ï¼šæ•°æ®åº“å­—æ®µåä¸æå–å˜é‡åä¸€è‡´
                    fh_raw = {
                        "revenue_ttm": revenue,
                        "net_profit_ttm": net_profit,  # åŸæœ‰æ ‡å‡†
                        "net_income_ttm": net_profit,  # æ–°å¢å…¼å®¹
                        "currency": currency,
                        "operating_cashflow_ttm": op_cf,
                        "free_cashflow_ttm": free_cf,
                        "total_assets": t_assets,
                        "total_liabilities": t_liab,
                        "total_debt": t_debt,
                        "cash_and_equivalents": cash,
                        "net_debt": n_debt,
                        "debt_to_equity": d_e,
                        "interest_coverage": int_cov,
                        "current_ratio": curr_ratio,
                        "dividend_yield": div_yield,
                        "payout_ratio": payout,
                        "buyback_ratio": buyback
                    }
                    
                    # è·å–è¡¨å®é™…å­—æ®µä»¥é¿å…é”™è¯¯ (ä»…é™æ‰§è¡Œ ALTER åçš„å­—æ®µ)
                    cursor.execute("PRAGMA table_info(financial_history)")
                    fh_actual_cols = [r[1] for r in cursor.fetchall()]
                    fh_data = {k: v for k, v in fh_raw.items() if k in fh_actual_cols and v is not None}
                    
                    if fh_data:
                        fh_cols = ["asset_id", "report_date"] + list(fh_data.keys())
                        fh_vals = [asset_id, as_of_date] + list(fh_data.values())
                        fh_placeholders = ", ".join(["?"] * len(fh_vals))
                        fh_update = ", ".join([f"{k} = excluded.{k}" for k in fh_data.keys()])
                        
                        if mode == "incremental":
                            sql_fh = f"""
                                INSERT OR IGNORE INTO financial_history ({', '.join(fh_cols)})
                                VALUES ({fh_placeholders})
                            """
                        else:
                            sql_fh = f"""
                                INSERT INTO financial_history ({', '.join(fh_cols)})
                                VALUES ({fh_placeholders})
                                ON CONFLICT(asset_id, report_date) DO UPDATE SET {fh_update}
                            """
                        
                        cursor.execute(sql_fh, fh_vals)
                        inserted += 1
                        
                    # 2. Update/Insert financial_fundamentals (é€‚é…å…¶ç‹¬ç‰¹çš„ net_income_ttm å‘½å)
                    ff_raw = {
                        "revenue_ttm": revenue,
                        "net_income_ttm": net_profit, # å…³é”®ï¼šfundamentals ä½¿ç”¨ income
                        "operating_cashflow_ttm": op_cf, 
                        "free_cashflow_ttm": free_cf,
                        "total_assets": t_assets,
                        "total_liabilities": t_liab,
                        "total_debt": t_debt,
                        "cash_and_equivalents": cash,
                        "net_debt": n_debt,
                        "debt_to_equity": d_e,
                        "interest_coverage": int_cov,
                        "current_ratio": curr_ratio,
                        "dividend_yield": div_yield,
                        "payout_ratio": payout,
                        "buyback_ratio": buyback,
                        "currency": currency,
                        "data_source": "csv-import"
                    }
                    
                    cursor.execute("PRAGMA table_info(financial_fundamentals)")
                    ff_actual_cols = [r[1] for r in cursor.fetchall()]
                    ff_data = {k: v for k, v in ff_raw.items() if k in ff_actual_cols and v is not None}
                    
                    if ff_data:
                        ff_cols = ["asset_id", "as_of_date"] + list(ff_data.keys())
                        ff_vals = [asset_id, as_of_date] + list(ff_data.values())
                        ff_placeholders = ", ".join(["?"] * len(ff_vals))
                        ff_update = ", ".join([f"{k} = excluded.{k}" for k in ff_data.keys()])
                        
                        if mode == "incremental":
                            sql_ff = f"""
                                INSERT OR IGNORE INTO financial_fundamentals ({', '.join(ff_cols)})
                                VALUES ({ff_placeholders})
                            """
                        else:
                            sql_ff = f"""
                                INSERT INTO financial_fundamentals ({', '.join(ff_cols)})
                                VALUES ({ff_placeholders})
                                ON CONFLICT(asset_id, as_of_date) DO UPDATE SET {ff_update}
                            """
                        
                        cursor.execute(sql_ff, ff_vals)
                except Exception as row_e:
                    print(f"Error row: {row_e}")
                    errors += 1
            
            conn.commit()
            mode_label = "å…¨è¦†ç›–" if mode == "overwrite" else "å¢é‡æ·»åŠ "
            return True, f"âœ… è´¢æŠ¥æ•°æ®å¯¼å…¥å®Œæˆ (æ¨¡å¼: {mode_label})\n- æ–°å¢è®°å½•: {inserted}\n- æ›´æ–°è®°å½•: {updated}\n- é”™è¯¯: {errors}"
        finally:
            conn.close()
    except Exception as e:
        return False, f"è´¢æŠ¥ CSV è§£æå¤±è´¥: {str(e)}"
